This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
microsoft-mcp/.git/config
microsoft-mcp/.git/description
microsoft-mcp/.git/HEAD
microsoft-mcp/.git/hooks/applypatch-msg.sample
microsoft-mcp/.git/hooks/commit-msg.sample
microsoft-mcp/.git/hooks/fsmonitor-watchman.sample
microsoft-mcp/.git/hooks/post-update.sample
microsoft-mcp/.git/hooks/pre-applypatch.sample
microsoft-mcp/.git/hooks/pre-commit.sample
microsoft-mcp/.git/hooks/pre-merge-commit.sample
microsoft-mcp/.git/hooks/pre-push.sample
microsoft-mcp/.git/hooks/pre-rebase.sample
microsoft-mcp/.git/hooks/pre-receive.sample
microsoft-mcp/.git/hooks/prepare-commit-msg.sample
microsoft-mcp/.git/hooks/push-to-checkout.sample
microsoft-mcp/.git/hooks/sendemail-validate.sample
microsoft-mcp/.git/hooks/update.sample
microsoft-mcp/.git/info/exclude
microsoft-mcp/.git/logs/HEAD
microsoft-mcp/.git/logs/refs/heads/master
microsoft-mcp/.git/logs/refs/remotes/origin/HEAD
microsoft-mcp/.git/packed-refs
microsoft-mcp/.git/refs/heads/master
microsoft-mcp/.git/refs/remotes/origin/HEAD
microsoft-mcp/.gitignore
microsoft-mcp/authenticate.py
microsoft-mcp/pyproject.toml
microsoft-mcp/README.md
microsoft-mcp/src/microsoft_mcp/__init__.py
microsoft-mcp/src/microsoft_mcp/auth.py
microsoft-mcp/src/microsoft_mcp/graph.py
microsoft-mcp/src/microsoft_mcp/server.py
microsoft-mcp/src/microsoft_mcp/tools.py
microsoft-mcp/tests/test_integration.py
tools.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="microsoft-mcp/.git/config">
[core]
	repositoryformatversion = 0
	filemode = false
	bare = false
	logallrefupdates = true
	symlinks = false
	ignorecase = true
[remote "origin"]
	url = https://github.com/elyxlz/microsoft-mcp.git
	fetch = +refs/heads/*:refs/remotes/origin/*
[branch "master"]
	remote = origin
	merge = refs/heads/master
	vscode-merge-base = origin/master
</file>

<file path="microsoft-mcp/.git/description">
Unnamed repository; edit this file 'description' to name the repository.
</file>

<file path="microsoft-mcp/.git/HEAD">
ref: refs/heads/master
</file>

<file path="microsoft-mcp/.git/hooks/applypatch-msg.sample">
#!/bin/sh
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to "applypatch-msg".

. git-sh-setup
commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
:
</file>

<file path="microsoft-mcp/.git/hooks/commit-msg.sample">
#!/bin/sh
#
# An example hook script to check the commit log message.
# Called by "git commit" with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to "commit-msg".

# Uncomment the below to add a Signed-off-by line to the message.
# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
# hook is more suited to it.
#
# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"

# This example catches duplicate Signed-off-by lines.

test "" = "$(grep '^Signed-off-by: ' "$1" |
	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
	echo >&2 Duplicate Signed-off-by lines.
	exit 1
}
</file>

<file path="microsoft-mcp/.git/hooks/fsmonitor-watchman.sample">
#!/usr/bin/perl

use strict;
use warnings;
use IPC::Open2;

# An example hook script to integrate Watchman
# (https://facebook.github.io/watchman/) with git to speed up detecting
# new and modified files.
#
# The hook is passed a version (currently 2) and last update token
# formatted as a string and outputs to stdout a new update token and
# all files that have been modified since the update token. Paths must
# be relative to the root of the working tree and separated by a single NUL.
#
# To enable this hook, rename this file to "query-watchman" and set
# 'git config core.fsmonitor .git/hooks/query-watchman'
#
my ($version, $last_update_token) = @ARGV;

# Uncomment for debugging
# print STDERR "$0 $version $last_update_token\n";

# Check the hook interface version
if ($version ne 2) {
	die "Unsupported query-fsmonitor hook version '$version'.\n" .
	    "Falling back to scanning...\n";
}

my $git_work_tree = get_working_dir();

my $retry = 1;

my $json_pkg;
eval {
	require JSON::XS;
	$json_pkg = "JSON::XS";
	1;
} or do {
	require JSON::PP;
	$json_pkg = "JSON::PP";
};

launch_watchman();

sub launch_watchman {
	my $o = watchman_query();
	if (is_work_tree_watched($o)) {
		output_result($o->{clock}, @{$o->{files}});
	}
}

sub output_result {
	my ($clockid, @files) = @_;

	# Uncomment for debugging watchman output
	# open (my $fh, ">", ".git/watchman-output.out");
	# binmode $fh, ":utf8";
	# print $fh "$clockid\n@files\n";
	# close $fh;

	binmode STDOUT, ":utf8";
	print $clockid;
	print "\0";
	local $, = "\0";
	print @files;
}

sub watchman_clock {
	my $response = qx/watchman clock "$git_work_tree"/;
	die "Failed to get clock id on '$git_work_tree'.\n" .
		"Falling back to scanning...\n" if $? != 0;

	return $json_pkg->new->utf8->decode($response);
}

sub watchman_query {
	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
	or die "open2() failed: $!\n" .
	"Falling back to scanning...\n";

	# In the query expression below we're asking for names of files that
	# changed since $last_update_token but not from the .git folder.
	#
	# To accomplish this, we're using the "since" generator to use the
	# recency index to select candidate nodes and "fields" to limit the
	# output to file names only. Then we're using the "expression" term to
	# further constrain the results.
	my $last_update_line = "";
	if (substr($last_update_token, 0, 1) eq "c") {
		$last_update_token = "\"$last_update_token\"";
		$last_update_line = qq[\n"since": $last_update_token,];
	}
	my $query = <<"	END";
		["query", "$git_work_tree", {$last_update_line
			"fields": ["name"],
			"expression": ["not", ["dirname", ".git"]]
		}]
	END

	# Uncomment for debugging the watchman query
	# open (my $fh, ">", ".git/watchman-query.json");
	# print $fh $query;
	# close $fh;

	print CHLD_IN $query;
	close CHLD_IN;
	my $response = do {local $/; <CHLD_OUT>};

	# Uncomment for debugging the watch response
	# open ($fh, ">", ".git/watchman-response.json");
	# print $fh $response;
	# close $fh;

	die "Watchman: command returned no output.\n" .
	"Falling back to scanning...\n" if $response eq "";
	die "Watchman: command returned invalid output: $response\n" .
	"Falling back to scanning...\n" unless $response =~ /^\{/;

	return $json_pkg->new->utf8->decode($response);
}

sub is_work_tree_watched {
	my ($output) = @_;
	my $error = $output->{error};
	if ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {
		$retry--;
		my $response = qx/watchman watch "$git_work_tree"/;
		die "Failed to make watchman watch '$git_work_tree'.\n" .
		    "Falling back to scanning...\n" if $? != 0;
		$output = $json_pkg->new->utf8->decode($response);
		$error = $output->{error};
		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		# Uncomment for debugging watchman output
		# open (my $fh, ">", ".git/watchman-output.out");
		# close $fh;

		# Watchman will always return all files on the first query so
		# return the fast "everything is dirty" flag to git and do the
		# Watchman query just to get it over with now so we won't pay
		# the cost in git to look up each individual file.
		my $o = watchman_clock();
		$error = $output->{error};

		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		output_result($o->{clock}, ("/"));
		$last_update_token = $o->{clock};

		eval { launch_watchman() };
		return 0;
	}

	die "Watchman: $error.\n" .
	"Falling back to scanning...\n" if $error;

	return 1;
}

sub get_working_dir {
	my $working_dir;
	if ($^O =~ 'msys' || $^O =~ 'cygwin') {
		$working_dir = Win32::GetCwd();
		$working_dir =~ tr/\\/\//;
	} else {
		require Cwd;
		$working_dir = Cwd::cwd();
	}

	return $working_dir;
}
</file>

<file path="microsoft-mcp/.git/hooks/post-update.sample">
#!/bin/sh
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to "post-update".

exec git update-server-info
</file>

<file path="microsoft-mcp/.git/hooks/pre-applypatch.sample">
#!/bin/sh
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-applypatch".

. git-sh-setup
precommit="$(git rev-parse --git-path hooks/pre-commit)"
test -x "$precommit" && exec "$precommit" ${1+"$@"}
:
</file>

<file path="microsoft-mcp/.git/hooks/pre-commit.sample">
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git commit" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-commit".

if git rev-parse --verify HEAD >/dev/null 2>&1
then
	against=HEAD
else
	# Initial commit: diff against an empty tree object
	against=$(git hash-object -t tree /dev/null)
fi

# If you want to allow non-ASCII filenames set this variable to true.
allownonascii=$(git config --type=bool hooks.allownonascii)

# Redirect output to stderr.
exec 1>&2

# Cross platform projects tend to avoid non-ASCII filenames; prevent
# them from being added to the repository. We exploit the fact that the
# printable range starts at the space character and ends with tilde.
if [ "$allownonascii" != "true" ] &&
	# Note that the use of brackets around a tr range is ok here, (it's
	# even required, for portability to Solaris 10's /usr/bin/tr), since
	# the square bracket bytes happen to fall in the designated range.
	test $(git diff --cached --name-only --diff-filter=A -z $against |
	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
then
	cat <<\EOF
Error: Attempt to add a non-ASCII file name.

This can cause problems if you want to work with people on other platforms.

To be portable it is advisable to rename the file.

If you know what you are doing you can disable this check using:

  git config hooks.allownonascii true
EOF
	exit 1
fi

# If there are whitespace errors, print the offending file names and fail.
exec git diff-index --check --cached $against --
</file>

<file path="microsoft-mcp/.git/hooks/pre-merge-commit.sample">
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git merge" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message to
# stderr if it wants to stop the merge commit.
#
# To enable this hook, rename this file to "pre-merge-commit".

. git-sh-setup
test -x "$GIT_DIR/hooks/pre-commit" &&
        exec "$GIT_DIR/hooks/pre-commit"
:
</file>

<file path="microsoft-mcp/.git/hooks/pre-push.sample">
#!/bin/sh

# An example hook script to verify what is about to be pushed.  Called by "git
# push" after it has checked the remote status, but before anything has been
# pushed.  If this script exits with a non-zero status nothing will be pushed.
#
# This hook is called with the following parameters:
#
# $1 -- Name of the remote to which the push is being done
# $2 -- URL to which the push is being done
#
# If pushing without using a named remote those arguments will be equal.
#
# Information about the commits which are being pushed is supplied as lines to
# the standard input in the form:
#
#   <local ref> <local oid> <remote ref> <remote oid>
#
# This sample shows how to prevent push of commits where the log message starts
# with "WIP" (work in progress).

remote="$1"
url="$2"

zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')

while read local_ref local_oid remote_ref remote_oid
do
	if test "$local_oid" = "$zero"
	then
		# Handle delete
		:
	else
		if test "$remote_oid" = "$zero"
		then
			# New branch, examine all commits
			range="$local_oid"
		else
			# Update to existing branch, examine new commits
			range="$remote_oid..$local_oid"
		fi

		# Check for WIP commit
		commit=$(git rev-list -n 1 --grep '^WIP' "$range")
		if test -n "$commit"
		then
			echo >&2 "Found WIP commit in $local_ref, not pushing"
			exit 1
		fi
	fi
done

exit 0
</file>

<file path="microsoft-mcp/.git/hooks/pre-rebase.sample">
#!/bin/sh
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The "pre-rebase" hook is run just before "git rebase" starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#
# $1 -- the upstream the series was forked from.
# $2 -- the branch being rebased (or empty when rebasing the current branch).
#
# This sample shows how to prevent topic branches that are already
# merged to 'next' branch from getting rebased, because allowing it
# would result in rebasing already published history.

publish=next
basebranch="$1"
if test "$#" = 2
then
	topic="refs/heads/$2"
else
	topic=`git symbolic-ref HEAD` ||
	exit 0 ;# we do not interrupt rebasing detached HEAD
fi

case "$topic" in
refs/heads/??/*)
	;;
*)
	exit 0 ;# we do not interrupt others.
	;;
esac

# Now we are dealing with a topic branch being rebased
# on top of master.  Is it OK to rebase it?

# Does the topic really exist?
git show-ref -q "$topic" || {
	echo >&2 "No such branch $topic"
	exit 1
}

# Is topic fully merged to master?
not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
if test -z "$not_in_master"
then
	echo >&2 "$topic is fully merged to master; better remove it."
	exit 1 ;# we could allow it, but there is no point.
fi

# Is topic ever merged to next?  If so you should not be rebasing it.
only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
only_next_2=`git rev-list ^master           ${publish} | sort`
if test "$only_next_1" = "$only_next_2"
then
	not_in_topic=`git rev-list "^$topic" master`
	if test -z "$not_in_topic"
	then
		echo >&2 "$topic is already up to date with master"
		exit 1 ;# we could allow it, but there is no point.
	else
		exit 0
	fi
else
	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
	/usr/bin/perl -e '
		my $topic = $ARGV[0];
		my $msg = "* $topic has commits already merged to public branch:\n";
		my (%not_in_next) = map {
			/^([0-9a-f]+) /;
			($1 => 1);
		} split(/\n/, $ARGV[1]);
		for my $elem (map {
				/^([0-9a-f]+) (.*)$/;
				[$1 => $2];
			} split(/\n/, $ARGV[2])) {
			if (!exists $not_in_next{$elem->[0]}) {
				if ($msg) {
					print STDERR $msg;
					undef $msg;
				}
				print STDERR " $elem->[1]\n";
			}
		}
	' "$topic" "$not_in_next" "$not_in_master"
	exit 1
fi

<<\DOC_END

This sample hook safeguards topic branches that have been
published from being rewound.

The workflow assumed here is:

 * Once a topic branch forks from "master", "master" is never
   merged into it again (either directly or indirectly).

 * Once a topic branch is fully cooked and merged into "master",
   it is deleted.  If you need to build on top of it to correct
   earlier mistakes, a new topic branch is created by forking at
   the tip of the "master".  This is not strictly necessary, but
   it makes it easier to keep your history simple.

 * Whenever you need to test or publish your changes to topic
   branches, merge them into "next" branch.

The script, being an example, hardcodes the publish branch name
to be "next", but it is trivial to make it configurable via
$GIT_DIR/config mechanism.

With this workflow, you would want to know:

(1) ... if a topic branch has ever been merged to "next".  Young
    topic branches can have stupid mistakes you would rather
    clean up before publishing, and things that have not been
    merged into other branches can be easily rebased without
    affecting other people.  But once it is published, you would
    not want to rewind it.

(2) ... if a topic branch has been fully merged to "master".
    Then you can delete it.  More importantly, you should not
    build on top of it -- other people may already want to
    change things related to the topic as patches against your
    "master", so if you need further changes, it is better to
    fork the topic (perhaps with the same name) afresh from the
    tip of "master".

Let's look at this example:

		   o---o---o---o---o---o---o---o---o---o "next"
		  /       /           /           /
		 /   a---a---b A     /           /
		/   /               /           /
	       /   /   c---c---c---c B         /
	      /   /   /             \         /
	     /   /   /   b---b C     \       /
	    /   /   /   /             \     /
    ---o---o---o---o---o---o---o---o---o---o---o "master"


A, B and C are topic branches.

 * A has one fix since it was merged up to "next".

 * B has finished.  It has been fully merged up to "master" and "next",
   and is ready to be deleted.

 * C has not merged to "next" at all.

We would want to allow C to be rebased, refuse A, and encourage
B to be deleted.

To compute (1):

	git rev-list ^master ^topic next
	git rev-list ^master        next

	if these match, topic has not merged in next at all.

To compute (2):

	git rev-list master..topic

	if this is empty, it is fully merged to "master".

DOC_END
</file>

<file path="microsoft-mcp/.git/hooks/pre-receive.sample">
#!/bin/sh
#
# An example hook script to make use of push options.
# The example simply echoes all push options that start with 'echoback='
# and rejects all pushes when the "reject" push option is used.
#
# To enable this hook, rename this file to "pre-receive".

if test -n "$GIT_PUSH_OPTION_COUNT"
then
	i=0
	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
	do
		eval "value=\$GIT_PUSH_OPTION_$i"
		case "$value" in
		echoback=*)
			echo "echo from the pre-receive-hook: ${value#*=}" >&2
			;;
		reject)
			exit 1
		esac
		i=$((i + 1))
	done
fi
</file>

<file path="microsoft-mcp/.git/hooks/prepare-commit-msg.sample">
#!/bin/sh
#
# An example hook script to prepare the commit log message.
# Called by "git commit" with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to "prepare-commit-msg".

# This hook includes three examples. The first one removes the
# "# Please enter the commit message..." help message.
#
# The second includes the output of "git diff --name-status -r"
# into the message, just before the "git status" output.  It is
# commented because it doesn't cope with --amend or with squashed
# commits.
#
# The third example adds a Signed-off-by line to the message, that can
# still be edited.  This is rarely a good idea.

COMMIT_MSG_FILE=$1
COMMIT_SOURCE=$2
SHA1=$3

/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"

# case "$COMMIT_SOURCE,$SHA1" in
#  ,|template,)
#    /usr/bin/perl -i.bak -pe '
#       print "\n" . `git diff --cached --name-status -r`
# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
#  *) ;;
# esac

# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
# if test -z "$COMMIT_SOURCE"
# then
#   /usr/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
# fi
</file>

<file path="microsoft-mcp/.git/hooks/push-to-checkout.sample">
#!/bin/sh

# An example hook script to update a checked-out tree on a git push.
#
# This hook is invoked by git-receive-pack(1) when it reacts to git
# push and updates reference(s) in its repository, and when the push
# tries to update the branch that is currently checked out and the
# receive.denyCurrentBranch configuration variable is set to
# updateInstead.
#
# By default, such a push is refused if the working tree and the index
# of the remote repository has any difference from the currently
# checked out commit; when both the working tree and the index match
# the current commit, they are updated to match the newly pushed tip
# of the branch. This hook is to be used to override the default
# behaviour; however the code below reimplements the default behaviour
# as a starting point for convenient modification.
#
# The hook receives the commit with which the tip of the current
# branch is going to be updated:
commit=$1

# It can exit with a non-zero status to refuse the push (when it does
# so, it must not modify the index or the working tree).
die () {
	echo >&2 "$*"
	exit 1
}

# Or it can make any necessary changes to the working tree and to the
# index to bring them to the desired state when the tip of the current
# branch is updated to the new commit, and exit with a zero status.
#
# For example, the hook can simply run git read-tree -u -m HEAD "$1"
# in order to emulate git fetch that is run in the reverse direction
# with git push, as the two-tree form of git read-tree -u -m is
# essentially the same as git switch or git checkout that switches
# branches while keeping the local changes in the working tree that do
# not interfere with the difference between the branches.

# The below is a more-or-less exact translation to shell of the C code
# for the default behaviour for git's push-to-checkout hook defined in
# the push_to_deploy() function in builtin/receive-pack.c.
#
# Note that the hook will be executed from the repository directory,
# not from the working tree, so if you want to perform operations on
# the working tree, you will have to adapt your code accordingly, e.g.
# by adding "cd .." or using relative paths.

if ! git update-index -q --ignore-submodules --refresh
then
	die "Up-to-date check failed"
fi

if ! git diff-files --quiet --ignore-submodules --
then
	die "Working directory has unstaged changes"
fi

# This is a rough translation of:
#
#   head_has_history() ? "HEAD" : EMPTY_TREE_SHA1_HEX
if git cat-file -e HEAD 2>/dev/null
then
	head=HEAD
else
	head=$(git hash-object -t tree --stdin </dev/null)
fi

if ! git diff-index --quiet --cached --ignore-submodules $head --
then
	die "Working directory has staged changes"
fi

if ! git read-tree -u -m "$commit"
then
	die "Could not update working tree to new HEAD"
fi
</file>

<file path="microsoft-mcp/.git/hooks/sendemail-validate.sample">
#!/bin/sh

# An example hook script to validate a patch (and/or patch series) before
# sending it via email.
#
# The hook should exit with non-zero status after issuing an appropriate
# message if it wants to prevent the email(s) from being sent.
#
# To enable this hook, rename this file to "sendemail-validate".
#
# By default, it will only check that the patch(es) can be applied on top of
# the default upstream branch without conflicts in a secondary worktree. After
# validation (successful or not) of the last patch of a series, the worktree
# will be deleted.
#
# The following config variables can be set to change the default remote and
# remote ref that are used to apply the patches against:
#
#   sendemail.validateRemote (default: origin)
#   sendemail.validateRemoteRef (default: HEAD)
#
# Replace the TODO placeholders with appropriate checks according to your
# needs.

validate_cover_letter () {
	file="$1"
	# TODO: Replace with appropriate checks (e.g. spell checking).
	true
}

validate_patch () {
	file="$1"
	# Ensure that the patch applies without conflicts.
	git am -3 "$file" || return
	# TODO: Replace with appropriate checks for this patch
	# (e.g. checkpatch.pl).
	true
}

validate_series () {
	# TODO: Replace with appropriate checks for the whole series
	# (e.g. quick build, coding style checks, etc.).
	true
}

# main -------------------------------------------------------------------------

if test "$GIT_SENDEMAIL_FILE_COUNTER" = 1
then
	remote=$(git config --default origin --get sendemail.validateRemote) &&
	ref=$(git config --default HEAD --get sendemail.validateRemoteRef) &&
	worktree=$(mktemp --tmpdir -d sendemail-validate.XXXXXXX) &&
	git worktree add -fd --checkout "$worktree" "refs/remotes/$remote/$ref" &&
	git config --replace-all sendemail.validateWorktree "$worktree"
else
	worktree=$(git config --get sendemail.validateWorktree)
fi || {
	echo "sendemail-validate: error: failed to prepare worktree" >&2
	exit 1
}

unset GIT_DIR GIT_WORK_TREE
cd "$worktree" &&

if grep -q "^diff --git " "$1"
then
	validate_patch "$1"
else
	validate_cover_letter "$1"
fi &&

if test "$GIT_SENDEMAIL_FILE_COUNTER" = "$GIT_SENDEMAIL_FILE_TOTAL"
then
	git config --unset-all sendemail.validateWorktree &&
	trap 'git worktree remove -ff "$worktree"' EXIT &&
	validate_series
fi
</file>

<file path="microsoft-mcp/.git/hooks/update.sample">
#!/bin/sh
#
# An example hook script to block unannotated tags from entering.
# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to "update".
#
# Config
# ------
# hooks.allowunannotated
#   This boolean sets whether unannotated tags will be allowed into the
#   repository.  By default they won't be.
# hooks.allowdeletetag
#   This boolean sets whether deleting tags will be allowed in the
#   repository.  By default they won't be.
# hooks.allowmodifytag
#   This boolean sets whether a tag may be modified after creation. By default
#   it won't be.
# hooks.allowdeletebranch
#   This boolean sets whether deleting branches will be allowed in the
#   repository.  By default they won't be.
# hooks.denycreatebranch
#   This boolean sets whether remotely creating branches will be denied
#   in the repository.  By default this is allowed.
#

# --- Command line
refname="$1"
oldrev="$2"
newrev="$3"

# --- Safety check
if [ -z "$GIT_DIR" ]; then
	echo "Don't run this script from the command line." >&2
	echo " (if you want, you could supply GIT_DIR then run" >&2
	echo "  $0 <ref> <oldrev> <newrev>)" >&2
	exit 1
fi

if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
	exit 1
fi

# --- Config
allowunannotated=$(git config --type=bool hooks.allowunannotated)
allowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)
denycreatebranch=$(git config --type=bool hooks.denycreatebranch)
allowdeletetag=$(git config --type=bool hooks.allowdeletetag)
allowmodifytag=$(git config --type=bool hooks.allowmodifytag)

# check for no description
projectdesc=$(sed -e '1q' "$GIT_DIR/description")
case "$projectdesc" in
"Unnamed repository"* | "")
	echo "*** Project description file hasn't been set" >&2
	exit 1
	;;
esac

# --- Check types
# if $newrev is 0000...0000, it's a commit to delete a ref.
zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
if [ "$newrev" = "$zero" ]; then
	newrev_type=delete
else
	newrev_type=$(git cat-file -t $newrev)
fi

case "$refname","$newrev_type" in
	refs/tags/*,commit)
		# un-annotated tag
		short_refname=${refname##refs/tags/}
		if [ "$allowunannotated" != "true" ]; then
			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
			exit 1
		fi
		;;
	refs/tags/*,delete)
		# delete tag
		if [ "$allowdeletetag" != "true" ]; then
			echo "*** Deleting a tag is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/tags/*,tag)
		# annotated tag
		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
		then
			echo "*** Tag '$refname' already exists." >&2
			echo "*** Modifying a tag is not allowed in this repository." >&2
			exit 1
		fi
		;;
	refs/heads/*,commit)
		# branch
		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
			echo "*** Creating a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/heads/*,delete)
		# delete branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/remotes/*,commit)
		# tracking branch
		;;
	refs/remotes/*,delete)
		# delete tracking branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	*)
		# Anything else (is there anything else?)
		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
		exit 1
		;;
esac

# --- Finished
exit 0
</file>

<file path="microsoft-mcp/.git/info/exclude">
# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~
</file>

<file path="microsoft-mcp/.git/logs/HEAD">
0000000000000000000000000000000000000000 e267151b5f89c898a77666ae3734f349c79bb3f2 Paul <paultwizzy21@gmail.com> 1757178268 -0500	clone: from https://github.com/elyxlz/microsoft-mcp.git
</file>

<file path="microsoft-mcp/.git/logs/refs/heads/master">
0000000000000000000000000000000000000000 e267151b5f89c898a77666ae3734f349c79bb3f2 Paul <paultwizzy21@gmail.com> 1757178268 -0500	clone: from https://github.com/elyxlz/microsoft-mcp.git
</file>

<file path="microsoft-mcp/.git/logs/refs/remotes/origin/HEAD">
0000000000000000000000000000000000000000 e267151b5f89c898a77666ae3734f349c79bb3f2 Paul <paultwizzy21@gmail.com> 1757178268 -0500	clone: from https://github.com/elyxlz/microsoft-mcp.git
</file>

<file path="microsoft-mcp/.git/packed-refs">
# pack-refs with: peeled fully-peeled sorted 
e267151b5f89c898a77666ae3734f349c79bb3f2 refs/remotes/origin/master
</file>

<file path="microsoft-mcp/.git/refs/heads/master">
e267151b5f89c898a77666ae3734f349c79bb3f2
</file>

<file path="microsoft-mcp/.git/refs/remotes/origin/HEAD">
ref: refs/remotes/origin/master
</file>

<file path="microsoft-mcp/.gitignore">
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
.env
.venv
env/
venv/
ENV/
.python-version
.mypy_cache/
.pytest_cache/
.coverage
htmlcov/
.tox/
.cache
.ruff_cache/
*.log
.DS_Store
.idea/
.vscode/
*.swp
*.swo
*~
</file>

<file path="microsoft-mcp/authenticate.py">
#!/usr/bin/env python3
"""
Authenticate Microsoft accounts for use with Microsoft MCP.
Run this script to sign in to one or more Microsoft accounts.
"""

import os
import sys
from pathlib import Path

# Add src to path so we can import our modules
sys.path.insert(0, str(Path(__file__).parent / "src"))

from dotenv import load_dotenv
from microsoft_mcp import auth

# Load environment variables before anything else
load_dotenv()


def main():
    if not os.getenv("MICROSOFT_MCP_CLIENT_ID"):
        print("Error: MICROSOFT_MCP_CLIENT_ID environment variable is required")
        print("\nPlease set it in your .env file or environment:")
        print("export MICROSOFT_MCP_CLIENT_ID='your-app-id'")
        sys.exit(1)

    print("Microsoft MCP Authentication")
    print("============================\n")

    # List current accounts
    accounts = auth.list_accounts()
    if accounts:
        print("Currently authenticated accounts:")
        for i, account in enumerate(accounts, 1):
            print(f"{i}. {account.username} (ID: {account.account_id})")
        print()
    else:
        print("No accounts currently authenticated.\n")

    # Authenticate new account
    while True:
        choice = input("Do you want to authenticate a new account? (y/n): ").lower()
        if choice == "n":
            break
        elif choice == "y":
            try:
                # Use the new authentication function
                new_account = auth.authenticate_new_account()

                if new_account:
                    print("\n✓ Authentication successful!")
                    print(f"Signed in as: {new_account.username}")
                    print(f"Account ID: {new_account.account_id}")
                else:
                    print(
                        "\n✗ Authentication failed: Could not retrieve account information"
                    )
            except Exception as e:
                print(f"\n✗ Authentication failed: {e}")
                continue

            print()
        else:
            print("Please enter 'y' or 'n'")

    # Final account summary
    accounts = auth.list_accounts()
    if accounts:
        print("\nAuthenticated accounts summary:")
        print("==============================")
        for account in accounts:
            print(f"• {account.username}")
            print(f"  Account ID: {account.account_id}")

        print(
            "\nYou can use these account IDs with any MCP tool by passing account_id parameter."
        )
        print("Example: send_email(..., account_id='<account-id>')")
    else:
        print("\nNo accounts authenticated.")

    print("\nAuthentication complete!")


if __name__ == "__main__":
    main()
</file>

<file path="microsoft-mcp/pyproject.toml">
[project]
name = "microsoft-mcp"
version = "0.1.0"
description = "Microsoft Graph MCP server for Outlook, Calendar, and OneDrive with multi-account support"
readme = "README.md"
authors = [
    { name = "elyx", email = "elio@pascarelli.com" }
]
requires-python = ">=3.12"
dependencies = [
    "fastmcp>=2.8.0",
    "httpx>=0.28.1",
    "msal>=1.32.3",
    "python-dotenv>=1.1.0",
]

[project.scripts]
microsoft-mcp = "microsoft_mcp.server:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[dependency-groups]
dev = [
    "mcp>=1.9.3",
    "pytest>=8.4.0",
    "pytest-asyncio>=1.0.0",
]
</file>

<file path="microsoft-mcp/README.md">
# Microsoft MCP

Powerful MCP server for Microsoft Graph API - a complete AI assistant toolkit for Outlook, Calendar, OneDrive, and Contacts.

## Features

- **Email Management**: Read, send, reply, manage attachments, organize folders
- **Calendar Intelligence**: Create, update, check availability, respond to invitations
- **OneDrive Files**: Upload, download, browse with pagination
- **Contacts**: Search and list contacts from your address book
- **Multi-Account**: Support for multiple Microsoft accounts (personal, work, school)
- **Unified Search**: Search across emails, files, events, and people

## Quick Start with Claude Desktop

```bash
# Add Microsoft MCP server (replace with your Azure app ID)
claude mcp add microsoft-mcp -e MICROSOFT_MCP_CLIENT_ID=your-app-id-here -- uvx --from git+https://github.com/elyxlz/microsoft-mcp.git microsoft-mcp

# Start Claude Desktop
claude
```

### Usage Examples

```bash
# Email examples
> read my latest emails with full content
> reply to the email from John saying "I'll review this today"
> send an email with attachment to alice@example.com

# Calendar examples  
> show my calendar for next week
> check if I'm free tomorrow at 2pm
> create a meeting with Bob next Monday at 10am

# File examples
> list files in my OneDrive
> upload this report to OneDrive
> search for "project proposal" across all my files

# Multi-account
> list all my Microsoft accounts
> send email from my work account
```

## Available Tools

### Email Tools
- **`list_emails`** - List emails with optional body content
- **`get_email`** - Get specific email with attachments
- **`create_email_draft`** - Create email draft with attachments support
- **`send_email`** - Send email immediately with CC/BCC and attachments
- **`reply_to_email`** - Reply maintaining thread context
- **`reply_all_email`** - Reply to all recipients in thread
- **`update_email`** - Mark emails as read/unread
- **`move_email`** - Move emails between folders
- **`delete_email`** - Delete emails
- **`get_attachment`** - Get email attachment content
- **`search_emails`** - Search emails by query

### Calendar Tools
- **`list_events`** - List calendar events with details
- **`get_event`** - Get specific event details
- **`create_event`** - Create events with location and attendees
- **`update_event`** - Reschedule or modify events
- **`delete_event`** - Cancel events
- **`respond_event`** - Accept/decline/tentative response to invitations
- **`check_availability`** - Check free/busy times for scheduling
- **`search_events`** - Search calendar events

### Contact Tools
- **`list_contacts`** - List all contacts
- **`get_contact`** - Get specific contact details
- **`create_contact`** - Create new contact
- **`update_contact`** - Update contact information
- **`delete_contact`** - Delete contact
- **`search_contacts`** - Search contacts by query

### File Tools
- **`list_files`** - Browse OneDrive files and folders
- **`get_file`** - Download file content
- **`create_file`** - Upload files to OneDrive
- **`update_file`** - Update existing file content
- **`delete_file`** - Delete files or folders
- **`search_files`** - Search files in OneDrive

### Utility Tools
- **`unified_search`** - Search across emails, events, and files
- **`list_accounts`** - Show authenticated Microsoft accounts
- **`authenticate_account`** - Start authentication for a new Microsoft account
- **`complete_authentication`** - Complete the authentication process after entering device code

## Manual Setup

### 1. Azure App Registration

1. Go to [Azure Portal](https://portal.azure.com) → Microsoft Entra ID → App registrations
2. New registration → Name: `microsoft-mcp`
3. Supported account types: Personal + Work/School
4. Authentication → Allow public client flows: Yes
5. API permissions → Add these delegated permissions:
   - Mail.ReadWrite
   - Calendars.ReadWrite
   - Files.ReadWrite
   - Contacts.Read
   - People.Read
   - User.Read
6. Copy Application ID

### 2. Installation

```bash
git clone https://github.com/elyxlz/microsoft-mcp.git
cd microsoft-mcp
uv sync
```

### 3. Authentication

```bash
# Set your Azure app ID
export MICROSOFT_MCP_CLIENT_ID="your-app-id-here"

# Run authentication script
uv run authenticate.py

# Follow the prompts to authenticate your Microsoft accounts
```

### 4. Claude Desktop Configuration

Add to your Claude Desktop configuration:

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`  
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "microsoft": {
      "command": "uvx",
      "args": ["--from", "git+https://github.com/elyxlz/microsoft-mcp.git", "microsoft-mcp"],
      "env": {
        "MICROSOFT_MCP_CLIENT_ID": "your-app-id-here"
      }
    }
  }
}
```

Or for local development:

```json
{
  "mcpServers": {
    "microsoft": {
      "command": "uv",
      "args": ["--directory", "/path/to/microsoft-mcp", "run", "microsoft-mcp"],
      "env": {
        "MICROSOFT_MCP_CLIENT_ID": "your-app-id-here"
      }
    }
  }
}
```

## Multi-Account Support

All tools require an `account_id` parameter as the first argument:

```python
# List accounts to get IDs
accounts = list_accounts()
account_id = accounts[0]["account_id"]

# Use account for operations
send_email(account_id, "user@example.com", "Subject", "Body")
list_emails(account_id, limit=10, include_body=True)
create_event(account_id, "Meeting", "2024-01-15T10:00:00Z", "2024-01-15T11:00:00Z")
```

## Development

```bash
# Run tests
uv run pytest tests/ -v

# Type checking
uv run pyright

# Format code
uvx ruff format .

# Lint
uvx ruff check --fix --unsafe-fixes .
```

## Example: AI Assistant Scenarios

### Smart Email Management
```python
# Get account ID first
accounts = list_accounts()
account_id = accounts[0]["account_id"]

# List latest emails with full content
emails = list_emails(account_id, limit=10, include_body=True)

# Reply maintaining thread
reply_to_email(account_id, email_id, "Thanks for your message. I'll review and get back to you.")

# Forward with attachments
email = get_email(email_id, account_id)
attachments = [get_attachment(email_id, att["id"], account_id) for att in email["attachments"]]
send_email(account_id, "boss@company.com", f"FW: {email['subject']}", email["body"]["content"], attachments=attachments)
```

### Intelligent Scheduling
```python
# Get account ID first
accounts = list_accounts()
account_id = accounts[0]["account_id"]

# Check availability before scheduling
availability = check_availability(account_id, "2024-01-15T10:00:00Z", "2024-01-15T18:00:00Z", ["colleague@company.com"])

# Create meeting with details
create_event(
    account_id,
    "Project Review",
    "2024-01-15T14:00:00Z", 
    "2024-01-15T15:00:00Z",
    location="Conference Room A",
    body="Quarterly review of project progress",
    attendees=["colleague@company.com", "manager@company.com"]
)
```

## Security Notes

- Tokens are cached locally in `~/.microsoft_mcp_token_cache.json`
- Use app-specific passwords if you have 2FA enabled
- Only request permissions your app actually needs
- Consider using a dedicated app registration for production

## Troubleshooting

- **Authentication fails**: Check your CLIENT_ID is correct
- **"Need admin approval"**: Use `MICROSOFT_MCP_TENANT_ID=consumers` for personal accounts
- **Missing permissions**: Ensure all required API permissions are granted in Azure
- **Token errors**: Delete `~/.microsoft_mcp_token_cache.json` and re-authenticate

## License

MIT
</file>

<file path="microsoft-mcp/src/microsoft_mcp/__init__.py">
def main() -> None:
    print("Hello from microsoft-mcp!")
</file>

<file path="microsoft-mcp/src/microsoft_mcp/auth.py">
import os
import msal
import pathlib as pl
from typing import NamedTuple
from dotenv import load_dotenv

load_dotenv()

CACHE_FILE = pl.Path.home() / ".microsoft_mcp_token_cache.json"
SCOPES = ["https://graph.microsoft.com/.default"]


class Account(NamedTuple):
    username: str
    account_id: str


def _read_cache() -> str | None:
    try:
        return CACHE_FILE.read_text()
    except FileNotFoundError:
        return None


def _write_cache(content: str) -> None:
    CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    CACHE_FILE.write_text(content)


def get_app() -> msal.PublicClientApplication:
    client_id = os.getenv("MICROSOFT_MCP_CLIENT_ID")
    if not client_id:
        raise ValueError("MICROSOFT_MCP_CLIENT_ID environment variable is required")

    tenant_id = os.getenv("MICROSOFT_MCP_TENANT_ID", "common")
    authority = f"https://login.microsoftonline.com/{tenant_id}"

    cache = msal.SerializableTokenCache()
    cache_content = _read_cache()
    if cache_content:
        cache.deserialize(cache_content)

    app = msal.PublicClientApplication(
        client_id, authority=authority, token_cache=cache
    )

    return app


def get_token(account_id: str | None = None) -> str:
    app = get_app()

    accounts = app.get_accounts()
    account = None

    if account_id:
        account = next(
            (a for a in accounts if a["home_account_id"] == account_id), None
        )
    elif accounts:
        account = accounts[0]

    result = app.acquire_token_silent(SCOPES, account=account)

    if not result:
        flow = app.initiate_device_flow(scopes=SCOPES)
        if "user_code" not in flow:
            raise Exception(
                f"Failed to get device code: {flow.get('error_description', 'Unknown error')}"
            )
        verification_uri = flow.get(
            "verification_uri",
            flow.get("verification_url", "https://microsoft.com/devicelogin"),
        )
        print(
            f"\nTo authenticate:\n1. Visit {verification_uri}\n2. Enter code: {flow['user_code']}"
        )
        result = app.acquire_token_by_device_flow(flow)

    if "error" in result:
        raise Exception(
            f"Auth failed: {result.get('error_description', result['error'])}"
        )

    cache = app.token_cache
    if isinstance(cache, msal.SerializableTokenCache) and cache.has_state_changed:
        _write_cache(cache.serialize())

    return result["access_token"]


def list_accounts() -> list[Account]:
    app = get_app()
    return [
        Account(username=a["username"], account_id=a["home_account_id"])
        for a in app.get_accounts()
    ]


def authenticate_new_account() -> Account | None:
    """Authenticate a new account interactively"""
    app = get_app()

    flow = app.initiate_device_flow(scopes=SCOPES)
    if "user_code" not in flow:
        raise Exception(
            f"Failed to get device code: {flow.get('error_description', 'Unknown error')}"
        )

    print("\nTo authenticate:")
    print(
        f"1. Visit: {flow.get('verification_uri', flow.get('verification_url', 'https://microsoft.com/devicelogin'))}"
    )
    print(f"2. Enter code: {flow['user_code']}")
    print("3. Sign in with your Microsoft account")
    print("\nWaiting for authentication...")

    result = app.acquire_token_by_device_flow(flow)

    if "error" in result:
        raise Exception(
            f"Auth failed: {result.get('error_description', result['error'])}"
        )

    cache = app.token_cache
    if isinstance(cache, msal.SerializableTokenCache) and cache.has_state_changed:
        _write_cache(cache.serialize())

    # Get the newly added account
    accounts = app.get_accounts()
    if accounts:
        # Find the account that matches the token we just got
        for account in accounts:
            if (
                account.get("username", "").lower()
                == result.get("id_token_claims", {})
                .get("preferred_username", "")
                .lower()
            ):
                return Account(
                    username=account["username"], account_id=account["home_account_id"]
                )
        # If exact match not found, return the last account
        account = accounts[-1]
        return Account(
            username=account["username"], account_id=account["home_account_id"]
        )

    return None
</file>

<file path="microsoft-mcp/src/microsoft_mcp/graph.py">
import httpx
import time
from typing import Any, Iterator
from .auth import get_token

BASE_URL = "https://graph.microsoft.com/v1.0"
# 15 x 320 KiB = 4,915,200 bytes
UPLOAD_CHUNK_SIZE = 15 * 320 * 1024

_client = httpx.Client(timeout=30.0, follow_redirects=True)


def request(
    method: str,
    path: str,
    account_id: str | None = None,
    params: dict[str, Any] | None = None,
    json: dict[str, Any] | None = None,
    data: bytes | None = None,
    max_retries: int = 3,
) -> dict[str, Any] | None:
    headers = {
        "Authorization": f"Bearer {get_token(account_id)}",
    }

    if method == "GET":
        if "$search" in (params or {}):
            headers["Prefer"] = 'outlook.body-content-type="text"'
        elif "body" in (params or {}).get("$select", ""):
            headers["Prefer"] = 'outlook.body-content-type="text"'
    else:
        headers["Content-Type"] = (
            "application/json" if json else "application/octet-stream"
        )

    if params and (
        "$search" in params
        or "contains(" in params.get("$filter", "")
        or "/any(" in params.get("$filter", "")
    ):
        headers["ConsistencyLevel"] = "eventual"
        params.setdefault("$count", "true")

    retry_count = 0
    while retry_count <= max_retries:
        try:
            response = _client.request(
                method=method,
                url=f"{BASE_URL}{path}",
                headers=headers,
                params=params,
                json=json,
                content=data,
            )

            if response.status_code == 429:
                retry_after = int(response.headers.get("Retry-After", "5"))
                if retry_count < max_retries:
                    time.sleep(min(retry_after, 60))
                    retry_count += 1
                    continue

            if response.status_code >= 500 and retry_count < max_retries:
                wait_time = (2**retry_count) * 1
                time.sleep(wait_time)
                retry_count += 1
                continue

            response.raise_for_status()

            if response.content:
                return response.json()
            return None

        except httpx.HTTPStatusError as e:
            if retry_count < max_retries and e.response.status_code >= 500:
                wait_time = (2**retry_count) * 1
                time.sleep(wait_time)
                retry_count += 1
                continue
            raise

    return None


def request_paginated(
    path: str,
    account_id: str | None = None,
    params: dict[str, Any] | None = None,
    limit: int | None = None,
) -> Iterator[dict[str, Any]]:
    """Make paginated requests following @odata.nextLink"""
    items_returned = 0
    next_link = None

    while True:
        if next_link:
            result = request("GET", next_link.replace(BASE_URL, ""), account_id)
        else:
            result = request("GET", path, account_id, params=params)

        if not result:
            break

        if "value" in result:
            for item in result["value"]:
                if limit and items_returned >= limit:
                    return
                yield item
                items_returned += 1

        next_link = result.get("@odata.nextLink")
        if not next_link:
            break


def download_raw(
    path: str, account_id: str | None = None, max_retries: int = 3
) -> bytes:
    headers = {"Authorization": f"Bearer {get_token(account_id)}"}

    retry_count = 0
    while retry_count <= max_retries:
        try:
            response = _client.get(f"{BASE_URL}{path}", headers=headers)

            if response.status_code == 429:
                retry_after = int(response.headers.get("Retry-After", "5"))
                if retry_count < max_retries:
                    time.sleep(min(retry_after, 60))
                    retry_count += 1
                    continue

            if response.status_code >= 500 and retry_count < max_retries:
                wait_time = (2**retry_count) * 1
                time.sleep(wait_time)
                retry_count += 1
                continue

            response.raise_for_status()
            return response.content

        except httpx.HTTPStatusError as e:
            if retry_count < max_retries and e.response.status_code >= 500:
                wait_time = (2**retry_count) * 1
                time.sleep(wait_time)
                retry_count += 1
                continue
            raise

    raise ValueError("Failed to download file after all retries")


def _do_chunked_upload(
    upload_url: str,
    data: bytes,
    headers: dict[str, str],
) -> dict[str, Any]:
    """Internal helper for chunked uploads"""
    file_size = len(data)

    for i in range(0, file_size, UPLOAD_CHUNK_SIZE):
        chunk_start = i
        chunk_end = min(i + UPLOAD_CHUNK_SIZE, file_size)
        chunk = data[chunk_start:chunk_end]

        chunk_headers = headers.copy()
        chunk_headers["Content-Length"] = str(len(chunk))
        chunk_headers["Content-Range"] = (
            f"bytes {chunk_start}-{chunk_end - 1}/{file_size}"
        )

        retry_count = 0
        while retry_count <= 3:
            try:
                response = _client.put(upload_url, content=chunk, headers=chunk_headers)

                if response.status_code == 429:
                    retry_after = int(response.headers.get("Retry-After", "5"))
                    if retry_count < 3:
                        time.sleep(min(retry_after, 60))
                        retry_count += 1
                        continue

                response.raise_for_status()

                if response.status_code in (200, 201):
                    return response.json()
                break

            except httpx.HTTPStatusError as e:
                if retry_count < 3 and e.response.status_code >= 500:
                    time.sleep((2**retry_count) * 1)
                    retry_count += 1
                    continue
                raise

    raise ValueError("Upload completed but no final response received")


def create_upload_session(
    path: str,
    account_id: str | None = None,
    item_properties: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """Create an upload session for large files"""
    payload = {"item": item_properties or {}}
    result = request("POST", f"{path}/createUploadSession", account_id, json=payload)
    if not result:
        raise ValueError("Failed to create upload session")
    return result


def upload_large_file(
    path: str,
    data: bytes,
    account_id: str | None = None,
    item_properties: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """Upload a large file using upload sessions"""
    file_size = len(data)

    if file_size <= UPLOAD_CHUNK_SIZE:
        result = request("PUT", f"{path}/content", account_id, data=data)
        if not result:
            raise ValueError("Failed to upload file")
        return result

    session = create_upload_session(path, account_id, item_properties)
    upload_url = session["uploadUrl"]

    headers = {"Authorization": f"Bearer {get_token(account_id)}"}
    return _do_chunked_upload(upload_url, data, headers)


def create_mail_upload_session(
    message_id: str,
    attachment_item: dict[str, Any],
    account_id: str | None = None,
) -> dict[str, Any]:
    """Create an upload session for large mail attachments"""
    result = request(
        "POST",
        f"/me/messages/{message_id}/attachments/createUploadSession",
        account_id,
        json={"AttachmentItem": attachment_item},
    )
    if not result:
        raise ValueError("Failed to create mail attachment upload session")
    return result


def upload_large_mail_attachment(
    message_id: str,
    name: str,
    data: bytes,
    account_id: str | None = None,
    content_type: str = "application/octet-stream",
) -> dict[str, Any]:
    """Upload a large mail attachment using upload sessions"""
    file_size = len(data)

    attachment_item = {
        "attachmentType": "file",
        "name": name,
        "size": file_size,
        "contentType": content_type,
    }

    session = create_mail_upload_session(message_id, attachment_item, account_id)
    upload_url = session["uploadUrl"]

    headers = {"Authorization": f"Bearer {get_token(account_id)}"}
    return _do_chunked_upload(upload_url, data, headers)


def search_query(
    query: str,
    entity_types: list[str],
    account_id: str | None = None,
    limit: int = 50,
    fields: list[str] | None = None,
) -> Iterator[dict[str, Any]]:
    """Use the modern /search/query API endpoint"""
    payload = {
        "requests": [
            {
                "entityTypes": entity_types,
                "query": {"queryString": query},
                "size": min(limit, 25),
                "from": 0,
            }
        ]
    }

    if fields:
        payload["requests"][0]["fields"] = fields

    items_returned = 0

    while True:
        result = request("POST", "/search/query", account_id, json=payload)

        if not result or "value" not in result:
            break

        for response in result["value"]:
            if "hitsContainers" in response:
                for container in response["hitsContainers"]:
                    if "hits" in container:
                        for hit in container["hits"]:
                            if limit and items_returned >= limit:
                                return
                            yield hit["resource"]
                            items_returned += 1

        if "@odata.nextLink" in result:
            break

        has_more = False
        for response in result.get("value", []):
            for container in response.get("hitsContainers", []):
                if container.get("moreResultsAvailable"):
                    has_more = True
                    break

        if not has_more:
            break

        payload["requests"][0]["from"] += payload["requests"][0]["size"]
</file>

<file path="microsoft-mcp/src/microsoft_mcp/server.py">
import os
import sys
from .tools import mcp


def main() -> None:
    if not os.getenv("MICROSOFT_MCP_CLIENT_ID"):
        print(
            "Error: MICROSOFT_MCP_CLIENT_ID environment variable is required",
            file=sys.stderr,
        )
        sys.exit(1)

    mcp.run()


if __name__ == "__main__":
    main()
</file>

<file path="microsoft-mcp/src/microsoft_mcp/tools.py">
import base64
import datetime as dt
import pathlib as pl
from typing import Any
from fastmcp import FastMCP
from . import graph, auth

mcp = FastMCP("microsoft-mcp")

FOLDERS = {
    k.casefold(): v
    for k, v in {
        "inbox": "inbox",
        "sent": "sentitems",
        "drafts": "drafts",
        "deleted": "deleteditems",
        "junk": "junkemail",
        "archive": "archive",
    }.items()
}


@mcp.tool
def list_accounts() -> list[dict[str, str]]:
    """List all signed-in Microsoft accounts"""
    return [
        {"username": acc.username, "account_id": acc.account_id}
        for acc in auth.list_accounts()
    ]


@mcp.tool
def authenticate_account() -> dict[str, str]:
    """Authenticate a new Microsoft account using device flow authentication

    Returns authentication instructions and device code for the user to complete authentication.
    The user must visit the URL and enter the code to authenticate their Microsoft account.
    """
    app = auth.get_app()
    flow = app.initiate_device_flow(scopes=auth.SCOPES)

    if "user_code" not in flow:
        error_msg = flow.get("error_description", "Unknown error")
        raise Exception(f"Failed to get device code: {error_msg}")

    verification_url = flow.get(
        "verification_uri",
        flow.get("verification_url", "https://microsoft.com/devicelogin"),
    )

    return {
        "status": "authentication_required",
        "instructions": "To authenticate a new Microsoft account:",
        "step1": f"Visit: {verification_url}",
        "step2": f"Enter code: {flow['user_code']}",
        "step3": "Sign in with the Microsoft account you want to add",
        "step4": "After authenticating, use the 'complete_authentication' tool to finish the process",
        "device_code": flow["user_code"],
        "verification_url": verification_url,
        "expires_in": flow.get("expires_in", 900),
        "_flow_cache": str(flow),
    }


@mcp.tool
def complete_authentication(flow_cache: str) -> dict[str, str]:
    """Complete the authentication process after the user has entered the device code

    Args:
        flow_cache: The flow data returned from authenticate_account (the _flow_cache field)

    Returns:
        Account information if authentication was successful
    """
    import ast

    try:
        flow = ast.literal_eval(flow_cache)
    except (ValueError, SyntaxError):
        raise ValueError("Invalid flow cache data")

    app = auth.get_app()
    result = app.acquire_token_by_device_flow(flow)

    if "error" in result:
        error_msg = result.get("error_description", result["error"])
        if "authorization_pending" in error_msg:
            return {
                "status": "pending",
                "message": "Authentication is still pending. The user needs to complete the authentication process.",
                "instructions": "Please ensure you've visited the URL and entered the code, then try again.",
            }
        raise Exception(f"Authentication failed: {error_msg}")

    # Save the token cache
    cache = app.token_cache
    if isinstance(cache, auth.msal.SerializableTokenCache) and cache.has_state_changed:
        auth._write_cache(cache.serialize())

    # Get the newly added account
    accounts = app.get_accounts()
    if accounts:
        # Find the account that matches the token we just got
        for account in accounts:
            if (
                account.get("username", "").lower()
                == result.get("id_token_claims", {})
                .get("preferred_username", "")
                .lower()
            ):
                return {
                    "status": "success",
                    "username": account["username"],
                    "account_id": account["home_account_id"],
                    "message": f"Successfully authenticated {account['username']}",
                }
        # If exact match not found, return the last account
        account = accounts[-1]
        return {
            "status": "success",
            "username": account["username"],
            "account_id": account["home_account_id"],
            "message": f"Successfully authenticated {account['username']}",
        }

    return {
        "status": "error",
        "message": "Authentication succeeded but no account was found",
    }


@mcp.tool
def list_emails(
    account_id: str,
    folder: str = "inbox",
    limit: int = 10,
    include_body: bool = True,
) -> list[dict[str, Any]]:
    """List emails from specified folder"""
    folder_path = FOLDERS.get(folder.casefold(), folder)

    if include_body:
        select_fields = "id,subject,from,toRecipients,ccRecipients,receivedDateTime,hasAttachments,body,conversationId,isRead"
    else:
        select_fields = "id,subject,from,toRecipients,receivedDateTime,hasAttachments,conversationId,isRead"

    params = {
        "$top": min(limit, 100),
        "$select": select_fields,
        "$orderby": "receivedDateTime desc",
    }

    emails = list(
        graph.request_paginated(
            f"/me/mailFolders/{folder_path}/messages",
            account_id,
            params=params,
            limit=limit,
        )
    )

    return emails


@mcp.tool
def get_email(
    email_id: str,
    account_id: str,
    include_body: bool = True,
    body_max_length: int = 50000,
    include_attachments: bool = True,
) -> dict[str, Any]:
    """Get email details with size limits

    Args:
        email_id: The email ID
        account_id: The account ID
        include_body: Whether to include the email body (default: True)
        body_max_length: Maximum characters for body content (default: 50000)
        include_attachments: Whether to include attachment metadata (default: True)
    """
    params = {}
    if include_attachments:
        params["$expand"] = "attachments($select=id,name,size,contentType)"

    result = graph.request("GET", f"/me/messages/{email_id}", account_id, params=params)
    if not result:
        raise ValueError(f"Email with ID {email_id} not found")

    # Truncate body if needed
    if include_body and "body" in result and "content" in result["body"]:
        content = result["body"]["content"]
        if len(content) > body_max_length:
            result["body"]["content"] = (
                content[:body_max_length]
                + f"\n\n[Content truncated - {len(content)} total characters]"
            )
            result["body"]["truncated"] = True
            result["body"]["total_length"] = len(content)
    elif not include_body and "body" in result:
        del result["body"]

    # Remove attachment content bytes to reduce size
    if "attachments" in result and result["attachments"]:
        for attachment in result["attachments"]:
            if "contentBytes" in attachment:
                del attachment["contentBytes"]

    return result


@mcp.tool
def create_email_draft(
    account_id: str,
    to: str | list[str],
    subject: str,
    body: str,
    cc: str | list[str] | None = None,
    attachments: str | list[str] | None = None,
) -> dict[str, Any]:
    """Create an email draft with file path(s) as attachments"""
    to_list = [to] if isinstance(to, str) else to

    message = {
        "subject": subject,
        "body": {"contentType": "Text", "content": body},
        "toRecipients": [{"emailAddress": {"address": addr}} for addr in to_list],
    }

    if cc:
        cc_list = [cc] if isinstance(cc, str) else cc
        message["ccRecipients"] = [
            {"emailAddress": {"address": addr}} for addr in cc_list
        ]

    small_attachments = []
    large_attachments = []

    if attachments:
        # Convert single path to list
        attachment_paths = (
            [attachments] if isinstance(attachments, str) else attachments
        )
        for file_path in attachment_paths:
            path = pl.Path(file_path).expanduser().resolve()
            content_bytes = path.read_bytes()
            att_size = len(content_bytes)
            att_name = path.name

            if att_size < 3 * 1024 * 1024:
                small_attachments.append(
                    {
                        "@odata.type": "#microsoft.graph.fileAttachment",
                        "name": att_name,
                        "contentBytes": base64.b64encode(content_bytes).decode("utf-8"),
                    }
                )
            else:
                large_attachments.append(
                    {
                        "name": att_name,
                        "content_bytes": content_bytes,
                        "content_type": "application/octet-stream",
                    }
                )

    if small_attachments:
        message["attachments"] = small_attachments

    result = graph.request("POST", "/me/messages", account_id, json=message)
    if not result:
        raise ValueError("Failed to create email draft")

    message_id = result["id"]

    for att in large_attachments:
        graph.upload_large_mail_attachment(
            message_id,
            att["name"],
            att["content_bytes"],
            account_id,
            att.get("content_type", "application/octet-stream"),
        )

    return result


@mcp.tool
def send_email(
    account_id: str,
    to: str | list[str],
    subject: str,
    body: str,
    cc: str | list[str] | None = None,
    attachments: str | list[str] | None = None,
) -> dict[str, str]:
    """Send an email immediately with file path(s) as attachments"""
    to_list = [to] if isinstance(to, str) else to

    message = {
        "subject": subject,
        "body": {"contentType": "Text", "content": body},
        "toRecipients": [{"emailAddress": {"address": addr}} for addr in to_list],
    }

    if cc:
        cc_list = [cc] if isinstance(cc, str) else cc
        message["ccRecipients"] = [
            {"emailAddress": {"address": addr}} for addr in cc_list
        ]

    # Check if we have large attachments
    has_large_attachments = False
    processed_attachments = []

    if attachments:
        # Convert single path to list
        attachment_paths = (
            [attachments] if isinstance(attachments, str) else attachments
        )
        for file_path in attachment_paths:
            path = pl.Path(file_path).expanduser().resolve()
            content_bytes = path.read_bytes()
            att_size = len(content_bytes)
            att_name = path.name

            processed_attachments.append(
                {
                    "name": att_name,
                    "content_bytes": content_bytes,
                    "content_type": "application/octet-stream",
                    "size": att_size,
                }
            )

            if att_size >= 3 * 1024 * 1024:
                has_large_attachments = True

    if not has_large_attachments and processed_attachments:
        message["attachments"] = [
            {
                "@odata.type": "#microsoft.graph.fileAttachment",
                "name": att["name"],
                "contentBytes": base64.b64encode(att["content_bytes"]).decode("utf-8"),
            }
            for att in processed_attachments
        ]
        graph.request("POST", "/me/sendMail", account_id, json={"message": message})
        return {"status": "sent"}
    elif has_large_attachments:
        # Create draft first, then add large attachments, then send
        # We need to handle large attachments manually here
        to_list = [to] if isinstance(to, str) else to
        message = {
            "subject": subject,
            "body": {"contentType": "Text", "content": body},
            "toRecipients": [{"emailAddress": {"address": addr}} for addr in to_list],
        }
        if cc:
            cc_list = [cc] if isinstance(cc, str) else cc
            message["ccRecipients"] = [
                {"emailAddress": {"address": addr}} for addr in cc_list
            ]

        result = graph.request("POST", "/me/messages", account_id, json=message)
        if not result:
            raise ValueError("Failed to create email draft")

        message_id = result["id"]

        for att in processed_attachments:
            if att["size"] >= 3 * 1024 * 1024:
                graph.upload_large_mail_attachment(
                    message_id,
                    att["name"],
                    att["content_bytes"],
                    account_id,
                    att.get("content_type", "application/octet-stream"),
                )
            else:
                small_att = {
                    "@odata.type": "#microsoft.graph.fileAttachment",
                    "name": att["name"],
                    "contentBytes": base64.b64encode(att["content_bytes"]).decode(
                        "utf-8"
                    ),
                }
                graph.request(
                    "POST",
                    f"/me/messages/{message_id}/attachments",
                    account_id,
                    json=small_att,
                )

        graph.request("POST", f"/me/messages/{message_id}/send", account_id)
        return {"status": "sent"}
    else:
        graph.request("POST", "/me/sendMail", account_id, json={"message": message})
        return {"status": "sent"}


@mcp.tool
def update_email(
    email_id: str, updates: dict[str, Any], account_id: str
) -> dict[str, Any]:
    """Update email properties (isRead, categories, flag, etc.)"""
    result = graph.request(
        "PATCH", f"/me/messages/{email_id}", account_id, json=updates
    )
    if not result:
        raise ValueError(f"Failed to update email {email_id} - no response")
    return result


@mcp.tool
def delete_email(email_id: str, account_id: str) -> dict[str, str]:
    """Delete an email"""
    graph.request("DELETE", f"/me/messages/{email_id}", account_id)
    return {"status": "deleted"}


@mcp.tool
def move_email(
    email_id: str, destination_folder: str, account_id: str
) -> dict[str, Any]:
    """Move email to another folder"""
    folder_path = FOLDERS.get(destination_folder.casefold(), destination_folder)

    folders = graph.request("GET", "/me/mailFolders", account_id)
    folder_id = None

    if not folders:
        raise ValueError("Failed to retrieve mail folders")
    if "value" not in folders:
        raise ValueError(f"Unexpected folder response structure: {folders}")

    for folder in folders["value"]:
        if folder["displayName"].lower() == folder_path.lower():
            folder_id = folder["id"]
            break

    if not folder_id:
        raise ValueError(f"Folder '{destination_folder}' not found")

    payload = {"destinationId": folder_id}
    result = graph.request(
        "POST", f"/me/messages/{email_id}/move", account_id, json=payload
    )
    if not result:
        raise ValueError("Failed to move email - no response from server")
    if "id" not in result:
        raise ValueError(f"Failed to move email - unexpected response: {result}")
    return {"status": "moved", "new_id": result["id"]}


@mcp.tool
def reply_to_email(account_id: str, email_id: str, body: str) -> dict[str, str]:
    """Reply to an email (sender only)"""
    endpoint = f"/me/messages/{email_id}/reply"
    payload = {"message": {"body": {"contentType": "Text", "content": body}}}
    graph.request("POST", endpoint, account_id, json=payload)
    return {"status": "sent"}


@mcp.tool
def reply_all_email(account_id: str, email_id: str, body: str) -> dict[str, str]:
    """Reply to all recipients of an email"""
    endpoint = f"/me/messages/{email_id}/replyAll"
    payload = {"message": {"body": {"contentType": "Text", "content": body}}}
    graph.request("POST", endpoint, account_id, json=payload)
    return {"status": "sent"}


@mcp.tool
def list_events(
    account_id: str,
    days_ahead: int = 7,
    days_back: int = 0,
    include_details: bool = True,
) -> list[dict[str, Any]]:
    """List calendar events within specified date range, including recurring event instances"""
    now = dt.datetime.now(dt.timezone.utc)
    start = (now - dt.timedelta(days=days_back)).isoformat()
    end = (now + dt.timedelta(days=days_ahead)).isoformat()

    params = {
        "startDateTime": start,
        "endDateTime": end,
        "$orderby": "start/dateTime",
        "$top": 100,
    }

    if include_details:
        params["$select"] = (
            "id,subject,start,end,location,body,attendees,organizer,isAllDay,recurrence,onlineMeeting,seriesMasterId"
        )
    else:
        params["$select"] = "id,subject,start,end,location,organizer,seriesMasterId"

    # Use calendarView to get recurring event instances
    events = list(
        graph.request_paginated("/me/calendarView", account_id, params=params)
    )

    return events


@mcp.tool
def get_event(event_id: str, account_id: str) -> dict[str, Any]:
    """Get full event details"""
    result = graph.request("GET", f"/me/events/{event_id}", account_id)
    if not result:
        raise ValueError(f"Event with ID {event_id} not found")
    return result


@mcp.tool
def create_event(
    account_id: str,
    subject: str,
    start: str,
    end: str,
    location: str | None = None,
    body: str | None = None,
    attendees: str | list[str] | None = None,
    timezone: str = "UTC",
) -> dict[str, Any]:
    """Create a calendar event"""
    event = {
        "subject": subject,
        "start": {"dateTime": start, "timeZone": timezone},
        "end": {"dateTime": end, "timeZone": timezone},
    }

    if location:
        event["location"] = {"displayName": location}

    if body:
        event["body"] = {"contentType": "Text", "content": body}

    if attendees:
        attendees_list = [attendees] if isinstance(attendees, str) else attendees
        event["attendees"] = [
            {"emailAddress": {"address": a}, "type": "required"} for a in attendees_list
        ]

    result = graph.request("POST", "/me/events", account_id, json=event)
    if not result:
        raise ValueError("Failed to create event")
    return result


@mcp.tool
def update_event(
    event_id: str, updates: dict[str, Any], account_id: str
) -> dict[str, Any]:
    """Update event properties"""
    formatted_updates = {}

    if "subject" in updates:
        formatted_updates["subject"] = updates["subject"]
    if "start" in updates:
        formatted_updates["start"] = {
            "dateTime": updates["start"],
            "timeZone": updates.get("timezone", "UTC"),
        }
    if "end" in updates:
        formatted_updates["end"] = {
            "dateTime": updates["end"],
            "timeZone": updates.get("timezone", "UTC"),
        }
    if "location" in updates:
        formatted_updates["location"] = {"displayName": updates["location"]}
    if "body" in updates:
        formatted_updates["body"] = {"contentType": "Text", "content": updates["body"]}

    result = graph.request(
        "PATCH", f"/me/events/{event_id}", account_id, json=formatted_updates
    )
    return result or {"status": "updated"}


@mcp.tool
def delete_event(
    account_id: str, event_id: str, send_cancellation: bool = True
) -> dict[str, str]:
    """Delete or cancel a calendar event"""
    if send_cancellation:
        graph.request("POST", f"/me/events/{event_id}/cancel", account_id, json={})
    else:
        graph.request("DELETE", f"/me/events/{event_id}", account_id)
    return {"status": "deleted"}


@mcp.tool
def respond_event(
    account_id: str,
    event_id: str,
    response: str = "accept",
    message: str | None = None,
) -> dict[str, str]:
    """Respond to event invitation (accept, decline, tentativelyAccept)"""
    payload: dict[str, Any] = {"sendResponse": True}
    if message:
        payload["comment"] = message

    graph.request("POST", f"/me/events/{event_id}/{response}", account_id, json=payload)
    return {"status": response}


@mcp.tool
def check_availability(
    account_id: str,
    start: str,
    end: str,
    attendees: str | list[str] | None = None,
) -> dict[str, Any]:
    """Check calendar availability for scheduling"""
    me_info = graph.request("GET", "/me", account_id)
    if not me_info or "mail" not in me_info:
        raise ValueError("Failed to get user email address")
    schedules = [me_info["mail"]]
    if attendees:
        attendees_list = [attendees] if isinstance(attendees, str) else attendees
        schedules.extend(attendees_list)

    payload = {
        "schedules": schedules,
        "startTime": {"dateTime": start, "timeZone": "UTC"},
        "endTime": {"dateTime": end, "timeZone": "UTC"},
        "availabilityViewInterval": 30,
    }

    result = graph.request("POST", "/me/calendar/getSchedule", account_id, json=payload)
    if not result:
        raise ValueError("Failed to check availability")
    return result


@mcp.tool
def list_contacts(account_id: str, limit: int = 50) -> list[dict[str, Any]]:
    """List contacts"""
    params = {"$top": min(limit, 100)}

    contacts = list(
        graph.request_paginated("/me/contacts", account_id, params=params, limit=limit)
    )

    return contacts


@mcp.tool
def get_contact(contact_id: str, account_id: str) -> dict[str, Any]:
    """Get contact details"""
    result = graph.request("GET", f"/me/contacts/{contact_id}", account_id)
    if not result:
        raise ValueError(f"Contact with ID {contact_id} not found")
    return result


@mcp.tool
def create_contact(
    account_id: str,
    given_name: str,
    surname: str | None = None,
    email_addresses: str | list[str] | None = None,
    phone_numbers: dict[str, str] | None = None,
) -> dict[str, Any]:
    """Create a new contact"""
    contact: dict[str, Any] = {"givenName": given_name}

    if surname:
        contact["surname"] = surname

    if email_addresses:
        email_list = (
            [email_addresses] if isinstance(email_addresses, str) else email_addresses
        )
        contact["emailAddresses"] = [
            {"address": email, "name": f"{given_name} {surname or ''}".strip()}
            for email in email_list
        ]

    if phone_numbers:
        if "business" in phone_numbers:
            contact["businessPhones"] = [phone_numbers["business"]]
        if "home" in phone_numbers:
            contact["homePhones"] = [phone_numbers["home"]]
        if "mobile" in phone_numbers:
            contact["mobilePhone"] = phone_numbers["mobile"]

    result = graph.request("POST", "/me/contacts", account_id, json=contact)
    if not result:
        raise ValueError("Failed to create contact")
    return result


@mcp.tool
def update_contact(
    contact_id: str, updates: dict[str, Any], account_id: str
) -> dict[str, Any]:
    """Update contact information"""
    result = graph.request(
        "PATCH", f"/me/contacts/{contact_id}", account_id, json=updates
    )
    return result or {"status": "updated"}


@mcp.tool
def delete_contact(contact_id: str, account_id: str) -> dict[str, str]:
    """Delete a contact"""
    graph.request("DELETE", f"/me/contacts/{contact_id}", account_id)
    return {"status": "deleted"}


@mcp.tool
def list_files(
    account_id: str, path: str = "/", limit: int = 50
) -> list[dict[str, Any]]:
    """List files and folders in OneDrive"""
    endpoint = (
        "/me/drive/root/children"
        if path == "/"
        else f"/me/drive/root:/{path}:/children"
    )
    params = {
        "$top": min(limit, 100),
        "$select": "id,name,size,lastModifiedDateTime,folder,file,@microsoft.graph.downloadUrl",
    }

    items = list(
        graph.request_paginated(endpoint, account_id, params=params, limit=limit)
    )

    return [
        {
            "id": item["id"],
            "name": item["name"],
            "type": "folder" if "folder" in item else "file",
            "size": item.get("size", 0),
            "modified": item.get("lastModifiedDateTime"),
            "download_url": item.get("@microsoft.graph.downloadUrl"),
        }
        for item in items
    ]


@mcp.tool
def get_file(file_id: str, account_id: str, download_path: str) -> dict[str, Any]:
    """Download a file from OneDrive to local path"""
    import subprocess

    metadata = graph.request("GET", f"/me/drive/items/{file_id}", account_id)
    if not metadata:
        raise ValueError(f"File with ID {file_id} not found")

    download_url = metadata.get("@microsoft.graph.downloadUrl")
    if not download_url:
        raise ValueError("No download URL available for this file")

    try:
        subprocess.run(
            ["curl", "-L", "-o", download_path, download_url],
            check=True,
            capture_output=True,
        )

        return {
            "path": download_path,
            "name": metadata.get("name", "unknown"),
            "size_mb": round(metadata.get("size", 0) / (1024 * 1024), 2),
            "mime_type": metadata.get("file", {}).get("mimeType") if metadata else None,
        }
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Failed to download file: {e.stderr.decode()}")


@mcp.tool
def create_file(
    onedrive_path: str, local_file_path: str, account_id: str
) -> dict[str, Any]:
    """Upload a local file to OneDrive"""
    path = pl.Path(local_file_path).expanduser().resolve()
    data = path.read_bytes()
    result = graph.upload_large_file(
        f"/me/drive/root:/{onedrive_path}:", data, account_id
    )
    if not result:
        raise ValueError(f"Failed to create file at path: {onedrive_path}")
    return result


@mcp.tool
def update_file(file_id: str, local_file_path: str, account_id: str) -> dict[str, Any]:
    """Update OneDrive file content from a local file"""
    path = pl.Path(local_file_path).expanduser().resolve()
    data = path.read_bytes()
    result = graph.upload_large_file(f"/me/drive/items/{file_id}", data, account_id)
    if not result:
        raise ValueError(f"Failed to update file with ID: {file_id}")
    return result


@mcp.tool
def delete_file(file_id: str, account_id: str) -> dict[str, str]:
    """Delete a file or folder"""
    graph.request("DELETE", f"/me/drive/items/{file_id}", account_id)
    return {"status": "deleted"}


@mcp.tool
def get_attachment(
    email_id: str, attachment_id: str, save_path: str, account_id: str
) -> dict[str, Any]:
    """Download email attachment to a specified file path"""
    result = graph.request(
        "GET", f"/me/messages/{email_id}/attachments/{attachment_id}", account_id
    )

    if not result:
        raise ValueError("Attachment not found")

    if "contentBytes" not in result:
        raise ValueError("Attachment content not available")

    # Save attachment to file
    path = pl.Path(save_path).expanduser().resolve()
    path.parent.mkdir(parents=True, exist_ok=True)
    content_bytes = base64.b64decode(result["contentBytes"])
    path.write_bytes(content_bytes)

    return {
        "name": result.get("name", "unknown"),
        "content_type": result.get("contentType", "application/octet-stream"),
        "size": result.get("size", 0),
        "saved_to": str(path),
    }


@mcp.tool
def search_files(
    query: str,
    account_id: str,
    limit: int = 50,
) -> list[dict[str, Any]]:
    """Search for files in OneDrive using the modern search API."""
    items = list(graph.search_query(query, ["driveItem"], account_id, limit))

    return [
        {
            "id": item["id"],
            "name": item["name"],
            "type": "folder" if "folder" in item else "file",
            "size": item.get("size", 0),
            "modified": item.get("lastModifiedDateTime"),
            "download_url": item.get("@microsoft.graph.downloadUrl"),
        }
        for item in items
    ]


@mcp.tool
def search_emails(
    query: str,
    account_id: str,
    limit: int = 50,
    folder: str | None = None,
) -> list[dict[str, Any]]:
    """Search emails using the modern search API."""
    if folder:
        # For folder-specific search, use the traditional endpoint
        folder_path = FOLDERS.get(folder.casefold(), folder)
        endpoint = f"/me/mailFolders/{folder_path}/messages"

        params = {
            "$search": f'"{query}"',
            "$top": min(limit, 100),
            "$select": "id,subject,from,toRecipients,receivedDateTime,hasAttachments,body,conversationId,isRead",
        }

        return list(
            graph.request_paginated(endpoint, account_id, params=params, limit=limit)
        )

    return list(graph.search_query(query, ["message"], account_id, limit))


@mcp.tool
def search_events(
    query: str,
    account_id: str,
    days_ahead: int = 365,
    days_back: int = 365,
    limit: int = 50,
) -> list[dict[str, Any]]:
    """Search calendar events using the modern search API."""
    events = list(graph.search_query(query, ["event"], account_id, limit))

    # Filter by date range if needed
    if days_ahead != 365 or days_back != 365:
        now = dt.datetime.now(dt.timezone.utc)
        start = now - dt.timedelta(days=days_back)
        end = now + dt.timedelta(days=days_ahead)

        filtered_events = []
        for event in events:
            event_start = dt.datetime.fromisoformat(
                event.get("start", {}).get("dateTime", "").replace("Z", "+00:00")
            )
            event_end = dt.datetime.fromisoformat(
                event.get("end", {}).get("dateTime", "").replace("Z", "+00:00")
            )

            if event_start <= end and event_end >= start:
                filtered_events.append(event)

        return filtered_events

    return events


@mcp.tool
def search_contacts(
    query: str,
    account_id: str,
    limit: int = 50,
) -> list[dict[str, Any]]:
    """Search contacts. Uses traditional search since unified_search doesn't support contacts."""
    params = {
        "$search": f'"{query}"',
        "$top": min(limit, 100),
    }

    contacts = list(
        graph.request_paginated("/me/contacts", account_id, params=params, limit=limit)
    )

    return contacts


@mcp.tool
def unified_search(
    query: str,
    account_id: str,
    entity_types: list[str] | None = None,
    limit: int = 50,
) -> dict[str, list[dict[str, Any]]]:
    """Search across multiple Microsoft 365 resources using the modern search API

    entity_types can include: 'message', 'event', 'drive', 'driveItem', 'list', 'listItem', 'site'
    If not specified, searches across all available types.
    """
    if not entity_types:
        entity_types = ["message", "event", "driveItem"]

    results = {entity_type: [] for entity_type in entity_types}

    items = list(graph.search_query(query, entity_types, account_id, limit))

    for item in items:
        resource_type = item.get("@odata.type", "").split(".")[-1]

        if resource_type == "message":
            results.setdefault("message", []).append(item)
        elif resource_type == "event":
            results.setdefault("event", []).append(item)
        elif resource_type in ["driveItem", "file", "folder"]:
            results.setdefault("driveItem", []).append(item)
        else:
            results.setdefault("other", []).append(item)

    return {k: v for k, v in results.items() if v}
</file>

<file path="microsoft-mcp/tests/test_integration.py">
import os
import asyncio
import json
from datetime import datetime, timedelta, timezone
import pytest
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from dotenv import load_dotenv

load_dotenv()

if not os.getenv("MICROSOFT_MCP_CLIENT_ID"):
    pytest.fail("MICROSOFT_MCP_CLIENT_ID environment variable is required")


def parse_result(result, tool_name=None):
    """Helper to parse MCP tool results consistently"""
    if result.content and hasattr(result.content[0], "text"):
        text = result.content[0].text
        if text == "[]":
            return []
        data = json.loads(text)
        # FastMCP seems to unwrap single-element lists, so rewrap for consistency
        list_tools = {
            "list_accounts",
            "list_emails",
            "list_events",
            "list_contacts",
            "list_files",
        }
        if tool_name in list_tools and isinstance(data, dict):
            return [data]
        return data
    return []


async def get_session():
    """Get MCP session"""
    server_params = StdioServerParameters(
        command="uv",
        args=["run", "microsoft-mcp"],
        env={
            "MICROSOFT_MCP_CLIENT_ID": os.getenv("MICROSOFT_MCP_CLIENT_ID", ""),
            "MICROSOFT_MCP_TENANT_ID": os.getenv("MICROSOFT_MCP_TENANT_ID", "common"),
        },
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            yield session


async def get_account_info(session):
    """Get account info"""
    result = await session.call_tool("list_accounts", {})
    assert not result.isError
    accounts = parse_result(result, "list_accounts")
    assert accounts and len(accounts) > 0, (
        "No accounts found - please authenticate first"
    )

    return {"email": accounts[0]["username"], "account_id": accounts[0]["account_id"]}


@pytest.mark.asyncio
async def test_list_accounts():
    """Test list_accounts tool"""
    async for session in get_session():
        result = await session.call_tool("list_accounts", {})
        assert not result.isError
        accounts = parse_result(result, "list_accounts")
        assert accounts is not None
        assert len(accounts) > 0
        assert "username" in accounts[0]
        assert "account_id" in accounts[0]


@pytest.mark.asyncio
async def test_list_emails():
    """Test list_emails tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "list_emails",
            {
                "account_id": account_info["account_id"],
                "limit": 3,
                "include_body": True,
            },
        )
        assert not result.isError
        emails = parse_result(result, "list_emails")
        assert emails is not None
        if len(emails) > 0:
            assert "id" in emails[0]
            assert "subject" in emails[0]
            assert "body" in emails[0]


@pytest.mark.asyncio
async def test_list_emails_without_body():
    """Test list_emails tool without body"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "list_emails",
            {
                "account_id": account_info["account_id"],
                "limit": 3,
                "include_body": False,
            },
        )
        assert not result.isError
        emails = parse_result(result, "list_emails")
        assert emails is not None
        if len(emails) > 0:
            assert "body" not in emails[0]


@pytest.mark.asyncio
async def test_get_email():
    """Test get_email tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        list_result = await session.call_tool(
            "list_emails", {"account_id": account_info["account_id"], "limit": 1}
        )
        emails = parse_result(list_result, "list_emails")

        if emails and len(emails) > 0:
            email_id = emails[0].get("id")
            result = await session.call_tool(
                "get_email",
                {"email_id": email_id, "account_id": account_info["account_id"]},
            )
            assert not result.isError
            email_detail = parse_result(result)
            assert email_detail is not None
            assert "id" in email_detail
            assert email_detail.get("id") == email_id


@pytest.mark.asyncio
async def test_create_email_draft():
    """Test create_email tool as draft"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "create_email_draft",
            {
                "account_id": account_info["account_id"],
                "to": account_info["email"],
                "subject": "MCP Test Draft",
                "body": "This is a test draft email",
            },
        )
        assert not result.isError
        draft_data = parse_result(result)
        assert draft_data is not None
        assert "id" in draft_data

        draft_id = draft_data.get("id")
        delete_result = await session.call_tool(
            "delete_email",
            {"email_id": draft_id, "account_id": account_info["account_id"]},
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_update_email():
    """Test update_email tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        list_result = await session.call_tool(
            "list_emails", {"account_id": account_info["account_id"], "limit": 1}
        )
        emails = parse_result(list_result, "list_emails")

        if emails and len(emails) > 0:
            email_id = emails[0].get("id")
            original_read_state = emails[0].get("isRead", True)

            result = await session.call_tool(
                "update_email",
                {
                    "email_id": email_id,
                    "account_id": account_info["account_id"],
                    "updates": {"isRead": not original_read_state},
                },
            )
            assert not result.isError

            restore_result = await session.call_tool(
                "update_email",
                {
                    "email_id": email_id,
                    "account_id": account_info["account_id"],
                    "updates": {"isRead": original_read_state},
                },
            )
            assert not restore_result.isError


@pytest.mark.asyncio
async def test_delete_email():
    """Test delete_email tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        draft_result = await session.call_tool(
            "create_email_draft",
            {
                "account_id": account_info["account_id"],
                "to": account_info["email"],
                "subject": "MCP Test Delete",
                "body": "This email will be deleted",
            },
        )
        draft_data = parse_result(draft_result)
        if draft_data and "id" in draft_data:
            result = await session.call_tool(
                "delete_email",
                {
                    "email_id": draft_data.get("id"),
                    "account_id": account_info["account_id"],
                },
            )
            assert not result.isError
            delete_result = parse_result(result)
            assert delete_result is not None
            assert delete_result.get("status") == "deleted"


@pytest.mark.asyncio
async def test_move_email():
    """Test move_email tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        list_result = await session.call_tool(
            "list_emails",
            {"account_id": account_info["account_id"], "folder": "inbox", "limit": 1},
        )
        emails = parse_result(list_result, "list_emails")

        if emails and len(emails) > 0:
            email_id = emails[0].get("id")
            result = await session.call_tool(
                "move_email",
                {
                    "email_id": email_id,
                    "account_id": account_info["account_id"],
                    "destination_folder": "archive",
                },
            )
            assert not result.isError

            move_result = parse_result(result, "move_email")
            new_email_id = move_result.get("new_id", email_id)

            restore_result = await session.call_tool(
                "move_email",
                {
                    "email_id": new_email_id,
                    "account_id": account_info["account_id"],
                    "destination_folder": "inbox",
                },
            )
            assert not restore_result.isError


@pytest.mark.asyncio
async def test_reply_to_email():
    """Test reply_to_email tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        await asyncio.sleep(2)
        list_result = await session.call_tool(
            "list_emails", {"account_id": account_info["account_id"], "limit": 5}
        )
        emails = parse_result(list_result, "list_emails")

        test_email = None
        if emails:
            test_email = next(
                (e for e in emails if "MCP Test" in e.get("subject", "")),
                emails[0] if emails else None,
            )

        if test_email:
            result = await session.call_tool(
                "reply_to_email",
                {
                    "account_id": account_info["account_id"],
                    "email_id": test_email.get("id"),
                    "body": "This is a test reply",
                },
            )
            assert not result.isError
            reply_result = parse_result(result)
            assert reply_result is not None
            assert reply_result.get("status") == "sent"


@pytest.mark.asyncio
async def test_reply_all_email():
    """Test reply_all_email tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        await asyncio.sleep(2)
        list_result = await session.call_tool(
            "list_emails", {"account_id": account_info["account_id"], "limit": 5}
        )
        emails = parse_result(list_result, "list_emails")

        test_email = None
        if emails:
            test_email = next(
                (e for e in emails if "MCP Test" in e.get("subject", "")),
                emails[0] if emails else None,
            )

        if test_email:
            result = await session.call_tool(
                "reply_all_email",
                {
                    "account_id": account_info["account_id"],
                    "email_id": test_email.get("id"),
                    "body": "This is a test reply to all",
                },
            )
            assert not result.isError
            reply_result = parse_result(result)
            assert reply_result is not None
            assert reply_result.get("status") == "sent"


@pytest.mark.asyncio
async def test_list_events():
    """Test list_events tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "list_events",
            {
                "account_id": account_info["account_id"],
                "days_ahead": 14,
                "include_details": True,
            },
        )
        assert not result.isError
        events = parse_result(result, "list_events")
        assert events is not None
        if len(events) > 0:
            assert "id" in events[0]
            assert "subject" in events[0]
            assert "start" in events[0]
            assert "end" in events[0]


@pytest.mark.asyncio
async def test_get_event():
    """Test get_event tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        list_result = await session.call_tool(
            "list_events", {"account_id": account_info["account_id"], "days_ahead": 30}
        )
        events = parse_result(list_result, "list_events")

        if events and len(events) > 0:
            event_id = events[0].get("id")
            result = await session.call_tool(
                "get_event",
                {"event_id": event_id, "account_id": account_info["account_id"]},
            )
            assert not result.isError
            event_detail = parse_result(result)
            assert event_detail is not None
            assert "id" in event_detail
            assert event_detail.get("id") == event_id


@pytest.mark.asyncio
async def test_create_event():
    """Test create_event tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        start_time = datetime.now(timezone.utc) + timedelta(days=7)
        end_time = start_time + timedelta(hours=1)

        result = await session.call_tool(
            "create_event",
            {
                "account_id": account_info["account_id"],
                "subject": "MCP Integration Test Event",
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
                "location": "Virtual Meeting Room",
                "body": "This is a test event created by integration tests",
                "attendees": [account_info["email"]],
            },
        )
        assert not result.isError
        event_data = parse_result(result)
        assert event_data is not None
        assert "id" in event_data

        event_id = event_data.get("id")
        delete_result = await session.call_tool(
            "delete_event",
            {
                "account_id": account_info["account_id"],
                "event_id": event_id,
                "send_cancellation": False,
            },
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_update_event():
    """Test update_event tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        start_time = datetime.now(timezone.utc) + timedelta(days=8)
        end_time = start_time + timedelta(hours=1)

        create_result = await session.call_tool(
            "create_event",
            {
                "account_id": account_info["account_id"],
                "subject": "MCP Test Event for Update",
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
            },
        )
        event_data = parse_result(create_result)
        assert event_data is not None
        event_id = event_data.get("id")

        new_start = start_time + timedelta(hours=2)
        new_end = new_start + timedelta(hours=1)

        result = await session.call_tool(
            "update_event",
            {
                "event_id": event_id,
                "account_id": account_info["account_id"],
                "updates": {
                    "subject": "MCP Test Event (Updated)",
                    "start": new_start.isoformat(),
                    "end": new_end.isoformat(),
                    "location": "Conference Room B",
                },
            },
        )
        assert not result.isError

        delete_result = await session.call_tool(
            "delete_event",
            {
                "account_id": account_info["account_id"],
                "event_id": event_id,
                "send_cancellation": False,
            },
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_delete_event():
    """Test delete_event tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        start_time = datetime.now(timezone.utc) + timedelta(days=9)
        end_time = start_time + timedelta(hours=1)

        create_result = await session.call_tool(
            "create_event",
            {
                "account_id": account_info["account_id"],
                "subject": "MCP Test Event for Deletion",
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
            },
        )
        event_data = parse_result(create_result)
        assert event_data is not None
        event_id = event_data.get("id")

        result = await session.call_tool(
            "delete_event",
            {
                "account_id": account_info["account_id"],
                "event_id": event_id,
                "send_cancellation": False,
            },
        )
        assert not result.isError
        delete_result = parse_result(result)
        assert delete_result is not None
        assert delete_result.get("status") == "deleted"


@pytest.mark.asyncio
async def test_respond_event():
    """Test respond_event tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        list_result = await session.call_tool(
            "list_events", {"account_id": account_info["account_id"], "days_ahead": 30}
        )
        events = parse_result(list_result, "list_events")

        if events:
            invite_event = next(
                (e for e in events if e.get("attendees") and len(e["attendees"]) > 1),
                None,
            )
            if invite_event:
                result = await session.call_tool(
                    "respond_event",
                    {
                        "account_id": account_info["account_id"],
                        "event_id": invite_event.get("id"),
                        "response": "tentativelyAccept",
                        "message": "I might be able to attend",
                    },
                )
                if not result.isError:
                    response_result = parse_result(result)
                    assert response_result is not None
                    assert response_result.get("status") == "tentativelyAccept"


@pytest.mark.asyncio
async def test_check_availability():
    """Test check_availability tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        check_start = (
            (datetime.now(timezone.utc) + timedelta(days=1))
            .replace(hour=10, minute=0)
            .isoformat()
        )
        check_end = (
            (datetime.now(timezone.utc) + timedelta(days=1))
            .replace(hour=17, minute=0)
            .isoformat()
        )

        result = await session.call_tool(
            "check_availability",
            {
                "account_id": account_info["account_id"],
                "start": check_start,
                "end": check_end,
                "attendees": [account_info["email"]],
            },
        )
        assert not result.isError
        availability = parse_result(result)
        assert availability is not None


@pytest.mark.asyncio
async def test_list_contacts():
    """Test list_contacts tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "list_contacts", {"account_id": account_info["account_id"], "limit": 10}
        )
        assert not result.isError
        contacts = parse_result(result, "list_contacts")
        assert contacts is not None
        if len(contacts) > 0:
            assert "id" in contacts[0]


@pytest.mark.asyncio
async def test_get_contact():
    """Test get_contact tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        list_result = await session.call_tool(
            "list_contacts", {"account_id": account_info["account_id"], "limit": 1}
        )
        assert not list_result.isError
        contacts = parse_result(list_result, "list_contacts")
        if contacts and len(contacts) > 0:
            contact_id = contacts[0].get("id")
            result = await session.call_tool(
                "get_contact",
                {"contact_id": contact_id, "account_id": account_info["account_id"]},
            )
            assert not result.isError
            contact_detail = parse_result(result)
            assert contact_detail is not None
            assert "id" in contact_detail


@pytest.mark.asyncio
async def test_create_contact():
    """Test create_contact tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "create_contact",
            {
                "account_id": account_info["account_id"],
                "given_name": "MCP",
                "surname": "TestContact",
                "email_addresses": ["mcp.test@example.com"],
                "phone_numbers": {"mobile": "+1234567890"},
            },
        )
        assert not result.isError
        new_contact = parse_result(result)
        assert new_contact is not None
        assert "id" in new_contact

        contact_id = new_contact.get("id")
        delete_result = await session.call_tool(
            "delete_contact",
            {"contact_id": contact_id, "account_id": account_info["account_id"]},
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_update_contact():
    """Test update_contact tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        create_result = await session.call_tool(
            "create_contact",
            {
                "account_id": account_info["account_id"],
                "given_name": "MCPUpdate",
                "surname": "Test",
            },
        )
        assert not create_result.isError
        new_contact = parse_result(create_result)
        contact_id = new_contact.get("id")

        result = await session.call_tool(
            "update_contact",
            {
                "contact_id": contact_id,
                "account_id": account_info["account_id"],
                "updates": {"givenName": "MCPUpdated"},
            },
        )
        assert not result.isError

        delete_result = await session.call_tool(
            "delete_contact",
            {"contact_id": contact_id, "account_id": account_info["account_id"]},
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_delete_contact():
    """Test delete_contact tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        create_result = await session.call_tool(
            "create_contact",
            {
                "account_id": account_info["account_id"],
                "given_name": "MCPDelete",
                "surname": "Test",
            },
        )
        assert not create_result.isError
        new_contact = parse_result(create_result)
        contact_id = new_contact.get("id")

        result = await session.call_tool(
            "delete_contact",
            {"contact_id": contact_id, "account_id": account_info["account_id"]},
        )
        assert not result.isError
        delete_result = parse_result(result)
        assert delete_result is not None
        assert delete_result.get("status") == "deleted"


@pytest.mark.asyncio
async def test_list_files():
    """Test list_files tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "list_files", {"account_id": account_info["account_id"]}
        )
        assert not result.isError
        files = parse_result(result)
        assert files is not None
        if len(files) > 0:
            assert "id" in files[0]
            assert "name" in files[0]
            assert "type" in files[0]


@pytest.mark.asyncio
async def test_get_file():
    """Test get_file tool"""
    import tempfile

    async for session in get_session():
        account_info = await get_account_info(session)
        test_content = "Test file content"
        test_filename = f"mcp-test-get-{datetime.now().strftime('%Y%m%d-%H%M%S')}.txt"
        
        # Create a temporary local file
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as local_file:
            local_file.write(test_content)
            local_file_path = local_file.name
        
        try:
            create_result = await session.call_tool(
                "create_file",
                {
                    "account_id": account_info["account_id"],
                    "onedrive_path": test_filename,
                    "local_file_path": local_file_path,
                },
            )
        finally:
            # Clean up local file
            if os.path.exists(local_file_path):
                os.unlink(local_file_path)
        file_data = parse_result(create_result)
        file_id = file_data.get("id")

        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            tmp_path = tmp_file.name

        try:
            result = await session.call_tool(
                "get_file",
                {
                    "file_id": file_id,
                    "account_id": account_info["account_id"],
                    "download_path": tmp_path,
                },
            )
            assert not result.isError
            retrieved_file = parse_result(result)
            assert retrieved_file is not None
            assert "path" in retrieved_file
            assert retrieved_file["path"] == tmp_path
            assert "name" in retrieved_file
            assert retrieved_file["name"] == test_filename
            assert "size_mb" in retrieved_file

            with open(tmp_path, "r") as f:
                downloaded_content = f.read()
            assert downloaded_content == test_content

        finally:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

        delete_result = await session.call_tool(
            "delete_file",
            {"file_id": file_id, "account_id": account_info["account_id"]},
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_create_file():
    """Test create_file tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        test_content = f"MCP Integration Test\nTimestamp: {datetime.now().isoformat()}"
        test_filename = (
            f"mcp-test-create-{datetime.now().strftime('%Y%m%d-%H%M%S')}.txt"
        )
        
        # Create a temporary local file
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as local_file:
            local_file.write(test_content)
            local_file_path = local_file.name
        
        try:
            result = await session.call_tool(
                "create_file",
                {
                    "account_id": account_info["account_id"],
                    "onedrive_path": test_filename,
                    "local_file_path": local_file_path,
                },
            )
        finally:
            # Clean up local file
            if os.path.exists(local_file_path):
                os.unlink(local_file_path)
        assert not result.isError
        upload_result = parse_result(result)
        assert upload_result is not None
        assert "id" in upload_result

        file_id = upload_result.get("id")
        delete_result = await session.call_tool(
            "delete_file",
            {"file_id": file_id, "account_id": account_info["account_id"]},
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_update_file():
    """Test update_file tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        test_content = "Original content"
        test_filename = (
            f"mcp-test-update-{datetime.now().strftime('%Y%m%d-%H%M%S')}.txt"
        )
        
        # Create a temporary local file
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as local_file:
            local_file.write(test_content)
            local_file_path = local_file.name
        
        try:
            create_result = await session.call_tool(
                "create_file",
                {
                    "account_id": account_info["account_id"],
                    "onedrive_path": test_filename,
                    "local_file_path": local_file_path,
                },
            )
        finally:
            # Clean up local file
            if os.path.exists(local_file_path):
                os.unlink(local_file_path)
        file_data = parse_result(create_result)
        file_id = file_data.get("id")

        updated_content = f"Updated content at {datetime.now().isoformat()}"
        
        # Create a temporary local file with updated content
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as updated_file:
            updated_file.write(updated_content)
            updated_file_path = updated_file.name
        
        try:
            result = await session.call_tool(
                "update_file",
                {
                    "account_id": account_info["account_id"],
                    "file_id": file_id,
                    "local_file_path": updated_file_path,
                },
            )
        finally:
            # Clean up local file
            if os.path.exists(updated_file_path):
                os.unlink(updated_file_path)
        assert not result.isError

        delete_result = await session.call_tool(
            "delete_file",
            {"file_id": file_id, "account_id": account_info["account_id"]},
        )
        assert not delete_result.isError


@pytest.mark.asyncio
async def test_delete_file():
    """Test delete_file tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        test_content = "File to be deleted"
        test_filename = (
            f"mcp-test-delete-{datetime.now().strftime('%Y%m%d-%H%M%S')}.txt"
        )
        
        # Create a temporary local file
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as local_file:
            local_file.write(test_content)
            local_file_path = local_file.name
        
        try:
            create_result = await session.call_tool(
                "create_file",
                {
                    "account_id": account_info["account_id"],
                    "onedrive_path": test_filename,
                    "local_file_path": local_file_path,
                },
            )
        finally:
            # Clean up local file
            if os.path.exists(local_file_path):
                os.unlink(local_file_path)
        file_data = parse_result(create_result)
        file_id = file_data.get("id")

        result = await session.call_tool(
            "delete_file",
            {"file_id": file_id, "account_id": account_info["account_id"]},
        )
        assert not result.isError
        delete_result = parse_result(result)
        assert delete_result is not None
        assert delete_result.get("status") == "deleted"


@pytest.mark.asyncio
async def test_get_attachment():
    """Test get_attachment tool"""
    async for session in get_session():
        account_info = await get_account_info(session)

        # First create an email with an attachment
        import tempfile
        import os
        
        # Create a temporary directory and file with specific name
        temp_dir = tempfile.mkdtemp()
        temp_file_path = os.path.join(temp_dir, "test_file.txt")
        
        with open(temp_file_path, 'w') as f:
            f.write("This is a test attachment content")
        
        try:
            draft_result = await session.call_tool(
                "create_email_draft",
                {
                    "account_id": account_info["account_id"],
                    "to": account_info["email"],
                    "subject": "MCP Test Email with Attachment",
                    "body": "This email contains a test attachment",
                    "attachments": temp_file_path,  # Test with single path
                },
            )
        finally:
            # Clean up temp file and directory
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
            if os.path.exists(temp_dir):
                os.rmdir(temp_dir)
        assert not draft_result.isError
        draft_data = parse_result(draft_result)
        email_id = draft_data["id"]

        # Get the email to retrieve attachment details
        email_result = await session.call_tool(
            "get_email",
            {
                "email_id": email_id,
                "account_id": account_info["account_id"],
            },
        )
        email_detail = parse_result(email_result)

        assert email_detail.get("attachments"), "Email should have attachments"
        attachment = email_detail["attachments"][0]

        # Test getting the attachment
        with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as save_file:
            save_path = save_file.name
        
        try:
            result = await session.call_tool(
                "get_attachment",
                {
                    "email_id": email_id,
                    "account_id": account_info["account_id"],
                    "attachment_id": attachment["id"],
                    "save_path": save_path,
                },
            )
            assert not result.isError
            attachment_data = parse_result(result)
            assert attachment_data is not None
            assert attachment_data["name"] == "test_file.txt"
            assert "saved_to" in attachment_data
            assert attachment_data["saved_to"] == save_path
            
            # Verify file was saved
            assert os.path.exists(save_path)
            with open(save_path, 'r') as f:
                content = f.read()
                assert content == "This is a test attachment content"
        finally:
            # Clean up saved file
            if os.path.exists(save_path):
                os.unlink(save_path)

        # Clean up - delete the draft
        await session.call_tool(
            "delete_email",
            {
                "email_id": email_id,
                "account_id": account_info["account_id"],
            },
        )


@pytest.mark.asyncio
async def test_search_files():
    """Test search_files tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "search_files",
            {"account_id": account_info["account_id"], "query": "test", "limit": 5},
        )
        assert not result.isError
        search_results = parse_result(result)
        assert search_results is not None


@pytest.mark.asyncio
async def test_search_emails():
    """Test search_emails tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "search_emails",
            {"account_id": account_info["account_id"], "query": "test", "limit": 5},
        )
        assert not result.isError
        search_results = parse_result(result)
        assert search_results is not None


@pytest.mark.asyncio
async def test_search_events():
    """Test search_events tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "search_events",
            {"account_id": account_info["account_id"], "query": "meeting", "limit": 5},
        )
        assert not result.isError
        search_results = parse_result(result)
        assert search_results is not None


@pytest.mark.asyncio
async def test_search_contacts():
    """Test search_contacts tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        result = await session.call_tool(
            "search_contacts",
            {
                "account_id": account_info["account_id"],
                "query": account_info["email"].split("@")[0],
                "limit": 5,
            },
        )
        assert not result.isError
        search_results = parse_result(result)
        assert search_results is not None


@pytest.mark.asyncio
async def test_send_email():
    """Test send_email tool"""
    async for session in get_session():
        account_info = await get_account_info(session)
        await asyncio.sleep(2)

        result = await session.call_tool(
            "send_email",
            {
                "account_id": account_info["account_id"],
                "to": account_info["email"],
                "subject": f"MCP Test Send Email {datetime.now(timezone.utc).isoformat()}",
                "body": "This is a test email sent via send_email tool",
            },
        )
        assert not result.isError
        sent_result = parse_result(result)
        assert sent_result is not None
        assert sent_result.get("status") == "sent"


@pytest.mark.asyncio
async def test_unified_search():
    """Test unified_search tool"""
    async for session in get_session():
        account_info = await get_account_info(session)

        result = await session.call_tool(
            "unified_search",
            {
                "account_id": account_info["account_id"],
                "query": "test",
                "entity_types": ["message"],
                "limit": 10,
            },
        )
        assert not result.isError
        search_results = parse_result(result)
        assert search_results is not None
        assert isinstance(search_results, dict)
        # Results should be grouped by entity type
        if "message" in search_results:
            assert isinstance(search_results["message"], list)
</file>

<file path="tools.py">
"""This module provides example tools for web scraping and search functionality.

It includes a basic Tavily search function (as an example)

These tools are intended as free examples to get started. For production use,
consider implementing more robust and specialized tools tailored to your needs.
"""

from typing import Any, Callable, List, Optional, cast, Dict, Literal, Union
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import InjectedToolArg
from typing_extensions import Annotated
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any
from PyPDF2 import PdfWriter, PdfReader
from geopy import Nominatim
import math 
import httpx
from dateutil.parser import parse as parse_datetime
# Assuming fast_flights module is correctly installed and accessible
from fast_flights import FlightData, Passengers, Result, get_flights

import os
from app.react_agent.configuration import Configuration
import re
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from typing import List, Dict, Iterator
from langchain.schema import HumanMessage, AIMessage
from langchain_core.messages import AnyMessage, HumanMessage
from langchain.prompts.chat import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from pydantic import BaseModel, Field, validator

# LangChain Community
from langchain_community.document_loaders import NeedleLoader
from langchain_community.retrievers import NeedleRetriever
from datetime import datetime
import pyairbnb

# from crewai_tools import BaseTool
from typing import Optional
from os import environ
from langchain.tools import BaseTool, Tool
import requests
from pydantic import Field, BaseModel  
import logging
import aiohttp
import asyncio
from bs4 import BeautifulSoup

from dotenv import load_dotenv
load_dotenv()
      

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

#----------------------------------------------------------- Tools i have created -----------------------------------------------------------
# Tavily Search: Performs web searches using the Tavily search engine, providing accurate and trusted results for general queries.
# Weather Search: Provides weather information for a given location and date using the OpenWeatherMap API.
# Route Finder: Finds the optimal route between locations using the Google Maps API.
# Flight Search: Provides flight information between two locations, including airlines, prices, departure/arrival times, and more.
# Google Scholar Search: Provides academic research results from Google Scholar, including titles, links, snippets, publication info, citations, and versions.
# Booking Scraper: Scrapes hotel data from Booking.com based on destination, check-in/check-out dates, and other parameters.
# Address Validation: Uses Google Maps Address Validation API to validate and refine addresses.
# Google Maps Static Map: Generates a static map image using the Google Maps Static API.
# Google Maps Roads API: Calls the Google Maps Roads API for snap-to-roads, nearest-roads, and speed limits.
# Google Maps Time Zone: Calls the Google Maps Time Zone API to get time zone information for a location.
# Google Maps Places API: Calls the Google Maps Places API for text search and nearby search.
# Google Maps Find Place API: Calls the Google Maps Find Place API to find places by text query.
# Google Maps Place Details API: Calls the Google Maps Place Details API to get detailed information about a place.
# Google Maps Geolocation: Calls the Google Maps Geolocation API to estimate the device's location.
# Google Maps Geocoding: Calls the Google Maps Geocoding API to convert addresses to geographic coordinates.
# Google Maps Elevation: Calls the Google Maps Elevation API to get elevation data for locations.
# Google Maps Distance Matrix: Calls the Google Maps Distance Matrix API to get travel distance and time data.
# Google Maps Directions: Calls the Google Maps Directions API to get travel directions.
# Yelp Business Search: Calls the Yelp Business Search endpoint to find businesses.
# Yelp Phone Search: Calls the Yelp Phone Search endpoint to find businesses by phone number.
# Yelp Business Details: Calls the Yelp Business Details endpoint to get information about a business.
# Yelp Business Reviews: Calls the Yelp Business Reviews endpoint to get reviews for a business.
# Yelp Events Search: Calls the Yelp Events search endpoint to find local events.
# Yelp GraphQL: Calls the Yelp GraphQL endpoint with a user-provided query.
# YouTube Search: Calls the YouTube Data API's 'search' endpoint to find videos.
# YouTube Videos: Calls the YouTube Data API's 'videos' endpoint to get video details.
# YouTube CommentThreads: Calls the YouTube Data API's 'commentThreads' endpoint to get video comments.
# YouTube PlaylistItems: Calls the YouTube Data API's 'playlistItems' endpoint to get playlist items.
# docling_text_extractor: Extracts text from PDFs with OCR fallback.
# docling_table_extractor: Extracts structured tables from PDF documents.
# docling_full_processor: Comprehensive PDF processing with text, OCR, and table extraction.
# add_file_to_collection: Add a file to the Needle collection.
# search_collection: Search the Needle collection using a retrieval chain.




#----------------------------------------------------------------------------------------------------------------------------
# Define Input Schema# Define Input Schema
class SearchToolInput(BaseModel):
    query: str = Field(..., description="The search query to look up.")
    max_results: Optional[int] = Field(default=10, description="The maximum number of search results to return.")

# Define the Tool
class TavilySearchTool:
    def __init__(self, max_results: int = 10):
        self.max_results = max_results

    def search(self, query: str) -> Optional[List[Dict[str, Any]]]:
        """
        Perform a web search using the Tavily search engine.
        """
        try:
            # Initialize the Tavily search tool with the configured max_results
            search_tool = TavilySearchResults(max_results=self.max_results)

            # Perform the search (synchronously)
            result = search_tool.invoke({"query": query})

            # Return the search results
            return result
        except Exception as e:
            return {"error": str(e)}

tavily_search_tool = Tool(
    name="Tavily Search",
    func=TavilySearchTool().search,
    description="Performs web searches using the Tavily search engine, providing accurate and trusted results for general queries.",
    args_schema=SearchToolInput
)

#----------------------------------------------------------------------------------------------------------------------------

# Define Input Schema
class WeatherSearchInput(BaseModel):
    location: str = Field(..., description="The location to get weather information for (e.g., 'New York').")
    date: Optional[str] = Field(..., description="The date for the weather forecast in YYYY-MM-DD format.")

# Define the WeatherSearchTool class
class WeatherSearchTool:
    def __init__(self):
        self.api_key = os.getenv("OPENWEATHERMAP_API_KEY")
        self.base_url = "https://api.openweathermap.org/data/2.5"
        if not self.api_key:
            raise ValueError("OpenWeatherMap API key is missing.")

    def get_weather(self, input: WeatherSearchInput) -> str:
        try:
            if input.date:
                # Use forecast endpoint for future dates
                forecast_url = f"{self.base_url}/forecast"
                forecast_params = {
                    "q": input.location,
                    "appid": self.api_key,
                    "units": "metric"
                }
                response = requests.get(forecast_url, params=forecast_params)
                response.raise_for_status()
                forecast_data = response.json()

                target_date = datetime.strptime(input.date, "%Y-%m-%d").date()
                for forecast in forecast_data.get('list', []):
                    forecast_date = datetime.fromtimestamp(forecast['dt']).date()
                    if forecast_date == target_date:
                        weather = forecast['weather'][0]['description']
                        temp_min = forecast['main']['temp_min']
                        temp_max = forecast['main']['temp_max']
                        humidity = forecast['main']['humidity']
                        return (
                            f"Weather in {input.location} on {input.date}:\n"
                            f"Description: {weather}\n"
                            f"Temperature: {temp_min}°C to {temp_max}°C\n"
                            f"Humidity: {humidity}%"
                        )
                return f"No weather data found for {input.location} on {input.date}."
            else:
                # Use weather endpoint for current weather
                weather_url = f"{self.base_url}/weather"
                weather_params = {
                    "q": input.location,
                    "appid": self.api_key,
                    "units": "metric"
                }
                response = requests.get(weather_url, params=weather_params)
                response.raise_for_status()
                weather_data = response.json()

                weather = weather_data['weather'][0]['description']
                temp = weather_data['main']['temp']
                humidity = weather_data['main']['humidity']
                return (
                    f"Current weather in {input.location}:\n"
                    f"Description: {weather}\n"
                    f"Temperature: {temp}°C\n"
                    f"Humidity: {humidity}%"
                )
        except Exception as e:
            return f"An error occurred: {str(e)}"


# Define LangChain Tool without coroutine (since it's now synchronous)
weather_tool = Tool(
    name="Weather Search",
    func=WeatherSearchTool().get_weather,
    description="Provides weather information for a given location and date using the OpenWeatherMap API.",
    args_schema=WeatherSearchInput
)



# ---------------------------------------------------------------------------- #
#                      WHATSAPP CONNECTION AND FUNCTIONS                       #
# ---------------------------------------------------------------------------- #
# This section contains the core logic for connecting to and interacting with  #
# the WhatsApp API, as provided in the prompt.                                 #
# ---------------------------------------------------------------------------- #

class WhatsAppConnection:
    """Handles connection and authentication with the WPPConnect server."""
    def __init__(self):
        self.base_url = os.getenv("WPPCONNECT_BASE_URL")
        self.session = os.getenv("WPPCONNECT_SESSION_NAME")
        self.secret_key = os.getenv("WPPCONNECT_SECRET_KEY")
        self.token = os.getenv("WPPCONNECT_TOKEN")

        if not all([self.base_url, self.session, self.secret_key, self.token]):
            raise ValueError(
                "One or more WhatsApp environment variables are not set. "
                "Please set WPPCONNECT_BASE_URL, WPPCONNECT_SESSION_NAME, "
                "WPPCONNECT_SECRET_KEY, and WPPCONNECT_TOKEN."
            )
        self.base_url = self.base_url.rstrip("/")


    def __enter__(self):
        # The connection logic doesn't require a special setup for each call,
        # but the context manager pattern is kept for consistency.
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # No special teardown needed.
        pass


def send_message(message: str, phone_number: str) -> Dict:
    """Sends a WhatsApp text message to a specified phone number."""
    if not phone_number:
        raise ValueError("Missing 'phone_number'. This field is required.")

    try:
        with WhatsAppConnection() as conn:
            url = f"{conn.base_url}/api/{conn.session}/send-message"
            headers = {
                "Content-Type": "application/json; charset=utf-8",
                "Authorization": f"Bearer {conn.token}",
            }
            data = {"phone": phone_number, "message": message, "isGroup": False}
            
            logger.info(f"Sending message to {phone_number}...")
            response = requests.post(url, json=data, headers=headers)
            response.raise_for_status()
            logger.info("Message sent successfully.")
            return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"Error sending WhatsApp message: {e}")
        return {"status": "error", "message": str(e)}
    except ValueError as e:
        logger.error(f"Configuration error: {e}")
        return {"status": "error", "message": str(e)}


def send_voice(audio_path: str, phone_number: str) -> Dict:
    """Sends a WhatsApp voice message from an audio file to a specified phone number."""
    if not phone_number:
        raise ValueError("Missing 'phone_number'. This field is required.")
    if not audio_path:
        raise ValueError("Missing 'audio_path'. This field is required.")

    # Convert audio file to base64
    try:
        with open(audio_path, "rb") as audio_file:
            base64_audio = base64.b64encode(audio_file.read()).decode("utf-8")
    except FileNotFoundError:
        error_msg = f"Audio file not found at path: {audio_path}"
        logger.error(error_msg)
        return {"status": "error", "message": error_msg}
    except Exception as e:
        error_msg = f"Error reading audio file: {e}"
        logger.error(error_msg)
        return {"status": "error", "message": error_msg}

    try:
        with WhatsAppConnection() as conn:
            url = f"{conn.base_url}/api/{conn.session}/send-voice-base64"
            headers = {
                "Content-Type": "application/json; charset=utf-8",
                "Authorization": f"Bearer {conn.token}",
            }
            data = {
                "phone": phone_number,
                "isGroup": False,
                "base64Ptt": f"data:audio/mpeg;base64,{base64_audio}",
            }

            logger.info(f"Sending voice message from {audio_path} to {phone_number}...")
            response = requests.post(url, json=data, headers=headers)
            response.raise_for_status()
            logger.info("Voice message sent successfully.")
            return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"Error sending WhatsApp voice message: {e}")
        return {"status": "error", "message": str(e)}
    except ValueError as e:
        logger.error(f"Configuration error: {e}")
        return {"status": "error", "message": str(e)}


# ---------------------------------------------------------------------------- #
#                        LANGCHAIN STRUCTURED TOOL CREATION                    #
# ---------------------------------------------------------------------------- #
# This section defines the Pydantic models for input validation and creates   #
# the structured LangChain tools that an agent can use.                        #
# ---------------------------------------------------------------------------- #

## Send Text Message Tool
class SendMessageInput(BaseModel):
    """Input schema for the Send WhatsApp Message tool."""
    message: str = Field(..., description="The text message content to be sent.")
    phone_number: str = Field(..., description="The recipient's phone number in international format (e.g., 15551234567).")

send_whatsapp_message_tool = Tool(
    name="send_whatsapp_message",
    func=send_message,
    description="Use this tool to send a WhatsApp text message to a specific phone number. It requires the message content and the recipient's phone number.",
    args_schema=SendMessageInput
)

## Send Voice Message Tool
class SendVoiceMessageInput(BaseModel):
    """Input schema for the Send WhatsApp Voice Message tool."""
    audio_path: str = Field(..., description="The local file path to the audio file (e.g., /path/to/voice_note.mp3) to be sent as a voice message.")
    phone_number: str = Field(..., description="The recipient's phone number in international format (e.g., 15551234567).")

send_whatsapp_voice_tool = Tool(
    name="send_whatsapp_voice_message",
    func=send_voice,
    description="Use this tool to send a WhatsApp voice message. It requires the local file path of the audio and the recipient's phone number.",
    args_schema=SendVoiceMessageInput
)



#----------------------------------------------------------------------------------------------------------------------------

import googlemaps
from googlemaps.convert import decode_polyline


#----------------------------------------------------------------------------------------------------------------------------

import os
import requests
from typing import List, Dict, Any
from pydantic import BaseModel, Field

# Define Input Schema
# Define Input Schema
class FlightSearchInput(BaseModel):
    departure_id: str = Field(..., description="The departure airport code or location kgmid.")
    arrival_id: str = Field(..., description="The arrival airport code or location kgmid.")
    outbound_date: str = Field(..., description="The outbound date in YYYY-MM-DD format.")
    return_date: str = Field(..., description="The return date in YYYY-MM-DD format (optional for one-way flights).")
    currency: str = Field(default="USD", description="The currency for the flight prices.")
    hl: str = Field(default="en", description="The language for the search results.")
    adults: int = Field(default=1, description="The number of adult passengers.")
    children: int = Field(default=0, description="The number of child passengers.")
    infants_in_seat: int = Field(default=0, description="The number of infants in seat.")
    infants_on_lap: int = Field(default=0, description="The number of infants on lap.")
    travel_class: int = Field(default=1, description="The travel class (1: Economy, 2: Premium Economy, 3: Business, 4: First).")
    sort_by: int = Field(default=1, description="The sorting order of the results (1: Top flights, 2: Price, etc.).")
    deep_search: bool = Field(default=False, description="Enable deep search for more precise results.")

# Define the Tool
class GoogleFlightsSearchTool:
    def __init__(self):
        self.api_key = os.getenv("SERPAPI_API_KEY")
        if not self.api_key:
            raise ValueError("SerpApi API key is missing. Please set the SERPAPI_API_KEY environment variable.")
        self.base_url = "https://serpapi.com/search.json"

    def _extract_flight_details(self, flight: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "airlines": [leg["airline"] for leg in flight.get("flights", [])],
            "price": flight.get("price"),
            "departure_airport": flight.get("flights", [{}])[0].get("departure_airport", {}).get("name"),
            "arrival_airport": flight.get("flights", [{}])[-1].get("arrival_airport", {}).get("name"),
            "departure_time": flight.get("flights", [{}])[0].get("departure_airport", {}).get("time"),
            "arrival_time": flight.get("flights", [{}])[-1].get("arrival_airport", {}).get("time"),
            "total_duration": flight.get("total_duration"),
            "layovers": [
                {
                    "duration": layover.get("duration"),
                    "airport": layover.get("name"),
                    "overnight": layover.get("overnight", False),
                }
                for layover in flight.get("layovers", [])
            ],
            "travel_class": flight.get("flights", [{}])[0].get("travel_class"),
            "carbon_emissions": flight.get("carbon_emissions", {}).get("this_flight"),
            "booking_token": flight.get("booking_token"),
            "departure_token": flight.get("departure_token"),
        }

    async def search_flights(self, input: FlightSearchInput) -> Dict[str, Any]:
        params = {
            "engine": "google_flights",
            "departure_id": input.departure_id,
            "arrival_id": input.arrival_id,
            "outbound_date": input.outbound_date,
            "currency": input.currency,
            "hl": input.hl,
            "adults": input.adults,
            "children": input.children,
            "infants_in_seat": input.infants_in_seat,
            "infants_on_lap": input.infants_on_lap,
            "travel_class": input.travel_class,
            "sort_by": input.sort_by,
            "deep_search": "true" if input.deep_search else "false",
            "api_key": self.api_key,
        }
        
        if input.return_date:
            params["return_date"] = input.return_date

        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(self.base_url, params=params) as response:
                    response.raise_for_status()
                    results = await response.json()

                    best_flights = [self._extract_flight_details(flight) for flight in results.get("best_flights", [])]
                    other_flights = [self._extract_flight_details(flight) for flight in results.get("other_flights", [])]

                    return {
                        "best_flights": best_flights,
                        "other_flights": other_flights,
                        "search_metadata": results.get("search_metadata", {}),
                        "search_parameters": results.get("search_parameters", {}),
                    }
            except Exception as e:
                return {"error": f"An error occurred: {str(e)}"}

flight_tool_instance = GoogleFlightsSearchTool()

flight_tool = Tool(
    name="Flight Search",
    func=flight_tool_instance.search_flights,
    coroutine=flight_tool_instance.search_flights,
    description="Provides flight information between two locations, including airlines, prices, departure/arrival times, and more.",
    args_schema=FlightSearchInput
)
        
# flight_tool = Tool(
#     name="Flight Search",
#     func=GoogleFlightsSearchTool().search_flights,
#     description="Provides flight information between two locations, including airlines, prices, departure/arrival times, and more.",
#     args_schema=FlightSearchInput
# )


#-------------------------------------------------------------- Google Flights Scraper --------------------------------------------------------------

class FlightSearchInput_2(BaseModel):
    """
    Input schema for searching flights.
    """
    departure_airport: str = Field(..., description="The departure airport code (e.g., LAX).")
    arrival_airport: str = Field(..., description="The arrival airport code (e.g., NYC).")
    departure_date: str = Field(..., description="The departure date in YYYY-MM-DD format.")
    return_date: Optional[str] = Field(..., description="The return date in YYYY-MM-DD format (optional for one-way flights).")
    adults: int = Field(default=1, description="The number of adults.")
    children: int = Field(default=0, description="The number of children.")
    travel_class: str = Field(default="all", description="The travel class (economy, business, first, or all).")
    sort_by: str = Field(default="price", description="Sort results by (price, duration, departure, arrival).")


class GoogleFlightsTool:
    def __init__(self):
        pass

    def _sort_flights(self, flights: List[Dict], sort_by: str) -> List[Dict]:
        """
        Sort flights by price, duration, departure, or arrival.
        """
        if sort_by == "price":
            # Remove currency symbols and commas, then convert to float for sorting
            return sorted(
                flights, 
                key=lambda x: float(x.get("price", "0").replace('$', '').replace(',', ''))
            )
        elif sort_by == "duration":
            # Assuming duration is in format "X hr Y min"
            def duration_in_minutes(flight):
                parts = flight.get("duration", "0 hr 0 min").split()
                hours = int(parts[0]) if len(parts) > 0 else 0
                minutes = int(parts[2]) if len(parts) > 2 else 0
                return hours * 60 + minutes
            return sorted(flights, key=duration_in_minutes)
        elif sort_by == "departure":
            return sorted(
                flights, 
                key=lambda x: datetime.strptime(x.get("departure_time", ""), "%I:%M %p on %a, %b %d, %Y")
            )
        elif sort_by == "arrival":
            return sorted(
                flights, 
                key=lambda x: datetime.strptime(x.get("arrival_time", ""), "%I:%M %p on %a, %b %d, %Y")
            )
        return flights

    def _structure_flight_data(self, result: Result, from_airport: str, to_airport: str, year: int, travel_class: str) -> List[Dict]:
        """
        Structure flight data into a list of dictionaries, including airport codes and year in time strings.
        """
        flights = []
        for flight in result.flights:
            flight_dict = {
                "airline": flight.name,
                "departure_time": f"{flight.departure}, {year}",
                "arrival_time": f"{flight.arrival}, {year}",
                "departure_airport": from_airport,
                "arrival_airport": to_airport,
                "duration": flight.duration,
                "stops": flight.stops,
                "price": flight.price,
                "travel_class": travel_class,
            }
            flights.append(flight_dict)
        return flights

    async def search_flights(self, input: FlightSearchInput_2) -> List[Dict]:
        """
        Search for flights and return a list of dictionaries.
        """
        try:
            structured_output = []

            # Prepare Passengers object
            passengers_obj = Passengers(
                adults=input.adults,
                children=input.children,
                infants_in_seat=0,
                infants_on_lap=0
            )

            # Extract year from departure_date
            departure_year = int(input.departure_date.split('-')[0])

            # --- Departure Flights ---
            departure_flight_data = [
                FlightData(
                    date=input.departure_date,
                    from_airport=input.departure_airport,
                    to_airport=input.arrival_airport
                )
            ]

            departure_result: Result = get_flights(
                flight_data=departure_flight_data,
                trip="one-way",
                seat=input.travel_class,
                passengers=passengers_obj,
                fetch_mode="fallback",
            )

            departure_flights = self._structure_flight_data(
                departure_result, 
                from_airport=input.departure_airport, 
                to_airport=input.arrival_airport,
                year=departure_year,
                travel_class=input.travel_class
            )
            departure_flights = self._sort_flights(departure_flights, input.sort_by)

            structured_output.append({"departure flights": departure_flights})

            # --- Return Flights (if return_date is provided) ---
            if input.return_date:
                arrival_year = int(input.return_date.split('-')[0])
                arrival_flight_data = [
                    FlightData(
                        date=input.return_date,
                        from_airport=input.arrival_airport,
                        to_airport=input.departure_airport
                    )
                ]

                arrival_result: Result = get_flights(
                    flight_data=arrival_flight_data,
                    trip="one-way",
                    seat=input.travel_class,
                    passengers=passengers_obj,
                    fetch_mode="fallback",
                )

                arrival_flights = self._structure_flight_data(
                    arrival_result, 
                    from_airport=input.arrival_airport, 
                    to_airport=input.departure_airport,
                    year=arrival_year,
                    travel_class=input.travel_class
                )
                arrival_flights = self._sort_flights(arrival_flights, input.sort_by)

                structured_output.append({"arrival flights": arrival_flights})

            return structured_output

        except Exception as e:
            return [{"error": f"An error occurred: {str(e)}"}]

# Initialize the tool instance
google_flight_instance = GoogleFlightsTool()

# Create the tool with both sync and async capabilities
google_flights_tool = Tool(
    name="Flight Search",
    func=google_flight_instance.search_flights,
    coroutine=google_flight_instance.search_flights,
    description="Provides flight information between two locations, including airlines, prices, departure/arrival times, and more.",
    args_schema=FlightSearchInput_2
)


#----------------------------------------------------------------------------------------------------------------------------------
# Input schema definition
class GoogleFlightsToolSync:
    def __init__(self):
        pass

    def _sort_flights(self, flights: List[Dict], sort_by: str) -> List[Dict]:
        """
        Sort flights by price, duration, departure, or arrival.
        """
        if sort_by == "price":
            # Remove currency symbols and commas, then convert to float for sorting
            return sorted(
                flights, 
                key=lambda x: float(x.get("price", "0").replace('$', '').replace(',', ''))
            )
        elif sort_by == "duration":
            # Assuming duration is in format "X hr Y min"
            def duration_in_minutes(flight):
                parts = flight.get("duration", "0 hr 0 min").split()
                hours = int(parts[0]) if len(parts) > 0 else 0
                minutes = int(parts[2]) if len(parts) > 2 else 0
                return hours * 60 + minutes
            return sorted(flights, key=duration_in_minutes)
        elif sort_by == "departure":
            return sorted(
                flights, 
                key=lambda x: datetime.strptime(x.get("departure_time", ""), "%I:%M %p on %a, %b %d, %Y")
            )
        elif sort_by == "arrival":
            return sorted(
                flights, 
                key=lambda x: datetime.strptime(x.get("arrival_time", ""), "%I:%M %p on %a, %b %d, %Y")
            )
        return flights

    def _structure_flight_data(self, result: Result, from_airport: str, to_airport: str, year: int, travel_class: str) -> List[Dict]:
        """
        Structure flight data into a list of dictionaries, including airport codes and year in time strings.
        """
        flights = []
        for flight in result.flights:
            flight_dict = {
                "airline": flight.name,
                "departure_time": f"{flight.departure}, {year}",
                "arrival_time": f"{flight.arrival}, {year}",
                "departure_airport": from_airport,
                "arrival_airport": to_airport,
                "duration": flight.duration,
                "stops": flight.stops,
                "price": flight.price,
                "travel_class": travel_class,
            }
            flights.append(flight_dict)
        return flights

    def search_flights(self, input: FlightSearchInput_2) -> List[Dict]:
        """
        Search for flights and return a list of dictionaries.
        """
        try:
            structured_output = []

            # Prepare Passengers object
            passengers_obj = Passengers(
                adults=input.adults,
                children=input.children,
                infants_in_seat=0,
                infants_on_lap=0
            )

            # Extract year from departure_date
            departure_year = int(input.departure_date.split('-')[0])

            # --- Departure Flights ---
            departure_flight_data = [
                FlightData(
                    date=input.departure_date,
                    from_airport=input.departure_airport,
                    to_airport=input.arrival_airport
                )
            ]

            # Use travel class directly from input, defaults to economy if not specified
            departure_result: Result = get_flights(
                flight_data=departure_flight_data,
                trip="one-way",
                seat=input.travel_class,
                passengers=passengers_obj,
                fetch_mode="fallback",
            )

            departure_flights = self._structure_flight_data(
                departure_result, 
                from_airport=input.departure_airport, 
                to_airport=input.arrival_airport,
                year=departure_year,
                travel_class=input.travel_class
            )
            departure_flights = self._sort_flights(departure_flights, input.sort_by)

            structured_output.append({"departure flights": departure_flights})

            # --- Return Flights (if return_date is provided) ---
            if input.return_date:
                arrival_year = int(input.return_date.split('-')[0])
                arrival_flight_data = [
                    FlightData(
                        date=input.return_date,
                        from_airport=input.arrival_airport,
                        to_airport=input.departure_airport
                    )
                ]

                arrival_result: Result = get_flights(
                    flight_data=arrival_flight_data,
                    trip="one-way",
                    seat=input.travel_class,
                    passengers=passengers_obj,
                    fetch_mode="fallback",
                )

                arrival_flights = self._structure_flight_data(
                    arrival_result, 
                    from_airport=input.arrival_airport, 
                    to_airport=input.departure_airport,
                    year=arrival_year,
                    travel_class=input.travel_class
                )
                arrival_flights = self._sort_flights(arrival_flights, input.sort_by)

                structured_output.append({"arrival flights": arrival_flights})

            return structured_output

        except Exception as e:
            return [{"error": f"An error occurred: {str(e)}"}]

# Initialize the tool
google_flights_tool_sync = Tool(
    name="Flight Search",
    func=GoogleFlightsToolSync().search_flights,
    description="Provides flight information between two locations, including airlines, prices, departure/arrival times, and more.",
    args_schema=FlightSearchInput_2
)


# class GoogleFlightsToolSync:
#     def __init__(self):
#         pass

#     def _filter_flights_by_class(self, flights: List[Dict], travel_class: str) -> List[Dict]:
#         """
#         Filter flights by travel class.
#         """
#         if travel_class == "all":
#             return flights
#         return [
#             flight for flight in flights 
#             if flight.get("travel_class", "economy").lower() == travel_class.lower()
#         ]

#     def _sort_flights(self, flights: List[Dict], sort_by: str) -> List[Dict]:
#         """
#         Sort flights by price, duration, departure, or arrival.
#         """
#         if sort_by == "price":
#             # Remove currency symbols and commas, then convert to float for sorting
#             return sorted(
#                 flights, 
#                 key=lambda x: float(x.get("price", "0").replace('$', '').replace(',', ''))
#             )
#         elif sort_by == "duration":
#             # Assuming duration is in format "X hr Y min"
#             def duration_in_minutes(flight):
#                 parts = flight.get("duration", "0 hr 0 min").split()
#                 hours = int(parts[0]) if len(parts) > 0 else 0
#                 minutes = int(parts[2]) if len(parts) > 2 else 0
#                 return hours * 60 + minutes
#             return sorted(flights, key=duration_in_minutes)
#         elif sort_by == "departure":
#             return sorted(
#                 flights, 
#                 key=lambda x: datetime.strptime(x.get("departure_time", ""), "%I:%M %p on %a, %b %d, %Y")
#             )
#         elif sort_by == "arrival":
#             return sorted(
#                 flights, 
#                 key=lambda x: datetime.strptime(x.get("arrival_time", ""), "%I:%M %p on %a, %b %d, %Y")
#             )
#         return flights

#     def _structure_flight_data(self, result: Result, from_airport: str, to_airport: str, year: int) -> List[Dict]:
#         """
#         Structure flight data into a list of dictionaries, including airport codes and year in time strings.
#         """
#         flights = []
#         for flight in result.flights:
#             flight_dict = {
#                 "airline": flight.name,
#                 "departure_time": f"{flight.departure}, {year}",
#                 "arrival_time": f"{flight.arrival}, {year}",
#                 "departure_airport": from_airport,
#                 "arrival_airport": to_airport,
#                 "duration": flight.duration,
#                 "stops": flight.stops,
#                 "price": flight.price,
#                 "travel_class": getattr(flight, "seat", "economy"),  # Default to economy if seat not present
#             }
#             flights.append(flight_dict)
#         return flights

#     def search_flights(self, input: FlightSearchInput_2) -> List[Dict]:
#         """
#         Search for flights and return a list of dictionaries.
#         """
#         try:
#             structured_output = []

#             # Prepare Passengers object
#             passengers_obj = Passengers(
#                 adults=input.adults,
#                 children=input.children,
#                 infants_in_seat=0,
#                 infants_on_lap=0
#             )

#             # Extract year from departure_date
#             departure_year = int(input.departure_date.split('-')[0])

#             # --- Departure Flights ---
#             departure_flight_data = [
#                 FlightData(
#                     date=input.departure_date,
#                     from_airport=input.departure_airport,
#                     to_airport=input.arrival_airport
#                 )
#             ]

#             departure_result: Result = get_flights(
#                 flight_data=departure_flight_data,
#                 trip="one-way",
#                 seat=input.travel_class if input.travel_class != "all" else "economy",  # Default to economy if "all"
#                 passengers=passengers_obj,
#                 fetch_mode="fallback",
#             )

#             departure_flights = self._structure_flight_data(
#                 departure_result, 
#                 from_airport=input.departure_airport, 
#                 to_airport=input.arrival_airport,
#                 year=departure_year
#             )
#             departure_flights = self._filter_flights_by_class(departure_flights, input.travel_class)
#             departure_flights = self._sort_flights(departure_flights, input.sort_by)

#             structured_output.append({"departure flights": departure_flights})

#             # --- Arrival Flights (if return_date is provided) ---
#             if input.return_date:
#                 arrival_year = int(input.return_date.split('-')[0])
#                 arrival_flight_data = [
#                     FlightData(
#                         date=input.return_date,
#                         from_airport=input.arrival_airport,
#                         to_airport=input.departure_airport
#                     )
#                 ]

#                 arrival_result: Result = get_flights(
#                     flight_data=arrival_flight_data,
#                     trip="one-way",
#                     seat=input.travel_class if input.travel_class != "all" else "economy",
#                     passengers=passengers_obj,
#                     fetch_mode="fallback",
#                 )

#                 arrival_flights = self._structure_flight_data(
#                     arrival_result, 
#                     from_airport=input.arrival_airport, 
#                     to_airport=input.departure_airport,
#                     year=arrival_year
#                 )
#                 arrival_flights = self._filter_flights_by_class(arrival_flights, input.travel_class)
#                 arrival_flights = self._sort_flights(arrival_flights, input.sort_by)

#                 structured_output.append({"arrival flights": arrival_flights})

#             return structured_output

#         except Exception as e:
#             # Always return a list of dictionaries, even in case of error
#             return [{"error": f"An error occurred: {str(e)}"}]

# # Initialize the tool
# google_flights_tool_sync = Tool(
#     name="Flight Search",
#     func=GoogleFlightsToolSync().search_flights,
#     description="Provides flight information between two locations, including airlines, prices, departure/arrival times, and more.",
#     args_schema=FlightSearchInput_2
# )



#----------------------------------------------------------------------------------------------------------------------------
# Define Input Schema
class BookingSearchInput(BaseModel):
    location: str = Field(..., description="The destination city or location (e.g., 'London').")
    checkin_date: str = Field(..., description="The check-in date in YYYY-MM-DD format.")
    checkout_date: str = Field(..., description="The check-out date in YYYY-MM-DD format.")
    adults: int = Field(default=2, description="The number of adult guests.")
    rooms: int = Field(default=1, description="The number of rooms.")
    currency: str = Field(default="USD", description="The currency for the prices.")


class BookingScraperTool:
    def __init__(self):
        self.base_url = "https://www.booking.com/searchresults.html"

    async def search(self, input: BookingSearchInput) -> List[Dict]:
        """
        Scrape hotel data from Booking.com based on the provided input parameters asynchronously.
        """
        params = {
            'ss': input.location,
            'dest_type': 'city',
            'checkin': input.checkin_date,
            'checkout': input.checkout_date,
            'group_adults': input.adults,
            'no_rooms': input.rooms,
            'selected_currency': input.currency
        }

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        }

        async with aiohttp.ClientSession() as session:
            async with session.get(self.base_url, params=params, headers=headers) as response:
                if response.status != 200:
                    return [{"error": f"Failed to fetch data, status code {response.status}"}]
                html_content = await response.text()
                soup = BeautifulSoup(html_content, 'html.parser')

        results = []
        for card in soup.find_all('div', {'data-testid': 'property-card'}):
            try:
                name = card.find('div', {'data-testid': 'title'}).text.strip()

                # Handle multiple possible price selectors
                price_elem = None
                selectors = [
                    {'class': 'prco-valign-middle-helper'},
                    {'data-testid': 'price-and-discounted-price'},
                    {'data-id': 'price-box'}
                ]
                for selector in selectors:
                    price_elem = card.find(['span', 'div'], selector)
                    if price_elem:
                        break

                price = price_elem.text.strip() if price_elem else 'N/A'

                rating_elem = card.find('div', {'data-testid': 'review-score'})
                rating = rating_elem.text.strip() if rating_elem else 'N/A'

                link_element = card.find('a', {'data-testid': 'title-link'})
                link = link_element['href'] if link_element else 'N/A'
                if link != 'N/A' and not link.startswith('http'):
                    link = f"https://www.booking.com{link}"

                results.append({
                    'name': name,
                    'price': price,
                    'rating': rating,
                    'link': link
                })
            except Exception as e:
                print(f'Error parsing hotel card: {str(e)}')
                continue

        return results if results else [{"error": "No hotels found."}]


# ✅ Instantiate the scraper tool first
booking_scraper_instance = BookingScraperTool()

# ✅ Define the LangChain tool correctly
booking_tool = Tool(
    name="Booking Scraper",
    func=booking_scraper_instance.search,  # Pass instance method
    coroutine=booking_scraper_instance.search,  # Explicitly define the coroutine
    description="Scrapes hotel data from Booking.com based on destination, check-in/check-out dates, and other parameters.",
    args_schema=BookingSearchInput
)

#------------------------------------------------------------
# List of available tools
TOOLS: List[Callable[..., Any]] = [tavily_search_tool]
#------------------------------------------------------------

#---------------------------------------------------------- Places Tool----------------------------------------------------------

import os
import googlemaps
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any

class GoogleMapsPlacesInput(BaseModel):
    """
    Input schema for the Google Maps Places API tool (search, nearby, details, etc.).
    
    Note: This is a broad input; in practice, you might split this into specialized 
          tools for find_place, text_search, nearby_search, place_details, etc.
    """
    query: Optional[str] = Field(
        None,
        description="Text query to search for, e.g. 'pizza in New York'."
    )
    location: Optional[str] = Field(
        None,
        description="Lat/lng or 'place_id:...' for nearby search or find_place bias."
    )
    radius: Optional[int] = Field(
        None,
        description="Radius in meters for nearby or text search."
    )
    type: Optional[List[str]] = Field(  # Changed to List[str]
        None,
        description="List of types of place, e.g., ['restaurant', 'museum']."
    )
    language: Optional[str] = Field(
        None,
        description="Language code for the response."
    )
    min_price: Optional[int] = Field(
        0,
        description="Minimum price range (0 to 4)."
    )
    max_price: Optional[int] = Field(
        4,
        description="Maximum price range (0 to 4)."
    )
    open_now: Optional[bool] = Field(
        False,
        description="Whether to show only places open now."
    )
    rank_by: Optional[str] = Field(
        None,
        description="For nearby search: 'prominence' or 'distance'."
    )
    name: Optional[str] = Field(
        None,
        description="A term to be matched against place names."
    )
    page_token: Optional[str] = Field(
        None,
        description="Token for pagination of results."
    )
    # Additional: for place details
    place_id: Optional[str] = Field(
        None,
        description="Place ID for retrieving details."
    )
    fields: Optional[List[str]] = Field(
        None,
        description="List of place detail fields to return."
    )


class GoogleMapsPlacesTool:
    """
    A tool to call various Google Places methods via googlemaps.Client:
      - find_place(...)
      - places(...)
      - places_nearby(...)
      - place(...)
    """
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_MAPS_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_MAPS_API_KEY environment variable is missing.")
        self.base_url = "https://maps.googleapis.com/maps/api/place"

    async def run_places_search(
        self,
        query: Optional[str] = None,
        location: Optional[str] = None,
        radius: Optional[int] = None,
        type: Optional[List[str]] = None,
        language: Optional[str] = None,
        min_price: Optional[int] = 0,
        max_price: Optional[int] = 4,
        open_now: Optional[bool] = False,
        rank_by: Optional[str] = None,
        name: Optional[str] = None,
        page_token: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Example: text search (places) or nearby search if 'location' is set.
        """
        try:
            params = {
                "key": self.api_key,
                "query": query,
                "location": location,
                "radius": radius,
                "type": ",".join(type) if type else None,
                "language": language,
                "minprice": min_price,
                "maxprice": max_price,
                "opennow": open_now,
                "rankby": rank_by,
                "name": name,
                "pagetoken": page_token
            }
            params = {k: v for k, v in params.items() if v is not None}

            if location and rank_by == "distance":
                url = f"{self.base_url}/nearbysearch/json"
            elif location and radius:
                url = f"{self.base_url}/nearbysearch/json"
            else:
                url = f"{self.base_url}/textsearch/json"

            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
                return {"places_search_result": response.json()}
        except Exception as e:
            return {"error": str(e)}

    async def run_find_place(
        self,
        query: str,
        input_type: str = "textquery",
        fields: Optional[List[str]] = None,
        location_bias: Optional[str] = None,
        language: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Wraps googlemaps.Client.find_place(...)
        """
        try:
            params = {
                "key": self.api_key,
                "input": query,
                "inputtype": input_type,
                "fields": ",".join(fields) if fields else None,
                "locationbias": location_bias,
                "language": language
            }
            params = {k: v for k, v in params.items() if v is not None}

            url = f"{self.base_url}/findplacefromtext/json"

            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
                return {"find_place_result": response.json()}
        except Exception as e:
            return {"error": str(e)}

    async def run_place_details(
        self,
        place_id: str,
        fields: Optional[List[str]] = None,
        language: Optional[str] = None,
        reviews_no_translations: Optional[bool] = False,
        reviews_sort: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Wraps googlemaps.Client.place(...)
        """
        try:
            params = {
                "key": self.api_key,
                "place_id": place_id,
                "fields": ",".join(fields) if fields else None,
                "language": language,
                "reviews_no_translations": reviews_no_translations,
                "reviews_sort": reviews_sort
            }
            params = {k: v for k, v in params.items() if v is not None}

            url = f"{self.base_url}/details/json"

            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                response.raise_for_status()
                return {"place_details_result": response.json()}
        except Exception as e:
            return {"error": str(e)}


# ✅ Instantiate the tool once to reuse the same instance
google_maps_tool_instance = GoogleMapsPlacesTool()

# ✅ Correctly define the tools for async execution
google_places_tool = Tool(
    name="Google Maps Places API",
    func=google_maps_tool_instance.run_places_search,
    coroutine=google_maps_tool_instance.run_places_search,
    description="Calls the Google Maps Places API for text search and nearby search.",
    args_schema=GoogleMapsPlacesInput
)

google_find_place_tool = Tool(
    name="Google Maps Find Place API",
    func=google_maps_tool_instance.run_find_place,
    coroutine=google_maps_tool_instance.run_find_place,
    description="Calls the Google Maps Find Place API to find places by text query.",
    args_schema=GoogleMapsPlacesInput
)

google_place_details_tool = Tool(
    name="Google Maps Place Details API",
    func=google_maps_tool_instance.run_place_details,
    coroutine=google_maps_tool_instance.run_place_details,
    description="Calls the Google Maps Place Details API to get detailed information about a place.",
    args_schema=GoogleMapsPlacesInput
)


#---------------------------------------------------------- Tools Condition ----------------------------------------------------------

def flight_tools_condition(
    state: Union[list[AnyMessage], dict[str, Any], BaseModel],
    messages_key: str = "messages",
) -> Literal["flight_tools", "accomodation_node"]:
    """Use in the conditional_edge to route to the ToolNode if the last message

    has tool calls. Otherwise, route to the end.

    Args:
        state (Union[list[AnyMessage], dict[str, Any], BaseModel]): The state to check for
            tool calls. Must have a list of messages (MessageGraph) or have the
            "messages" key (StateGraph).

    Returns:
        The next node to route to.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):
        ai_message = messages[-1]
    elif messages := getattr(state, messages_key, []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "flight_tools"
    return "accomodation_node"    # you can change this to any other node name instead of "__end__"


def accomodation_tools_condition(
    state: Union[list[AnyMessage], dict[str, Any], BaseModel],
    messages_key: str = "messages",
) -> Literal["accomodation_tools", "activity_planner"]:
    """Use in the conditional_edge to route to the ToolNode if the last message

    has tool calls. Otherwise, route to the end.

    Args:
        state (Union[list[AnyMessage], dict[str, Any], BaseModel]): The state to check for
            tool calls. Must have a list of messages (MessageGraph) or have the
            "messages" key (StateGraph).

    Returns:
        The next node to route to.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):
        ai_message = messages[-1]
    elif messages := getattr(state, messages_key, []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "accomodation_tools"
    return "activity_planner"    # you can change this to any other node name instead of "__end__"


def activity_planner_tools_condition(
    state: Union[list[AnyMessage], dict[str, Any], BaseModel],
    messages_key: str = "messages",
) -> Literal["activity_planner_tools", "realtime_provider"]:
    """Use in the conditional_edge to route to the ToolNode if the last message

    has tool calls. Otherwise, route to the end.

    Args:
        state (Union[list[AnyMessage], dict[str, Any], BaseModel]): The state to check for
            tool calls. Must have a list of messages (MessageGraph) or have the
            "messages" key (StateGraph).

    Returns:
        The next node to route to.
    """
    if isinstance(state, list):
        ai_message = state[-1]
    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):
        ai_message = messages[-1]
    elif messages := getattr(state, messages_key, []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "activity_planner_tools"
    return "realtime_provider"    # you can change this to any other node name instead of "__end__"


#---------------------------------------------------------- TicketMaster ----------------------------------------------------------
class TicketmasterEventSearchInput(BaseModel):
    keyword: Optional[str] = Field(default=None, description="Keyword to search for events (e.g., artist, event name).")
    city: Optional[str] = Field(default=None, description="Filter events by city.")
    country_code: Optional[str] = Field(default=None, description="Filter events by country code (ISO Alpha-2 Code).")
    classification_name: Optional[str] = Field(default=None, description="Filter by classification (e.g., 'Music').")
    start_date_time: Optional[str] = Field(default=None, description="Start date filter in ISO8601 format (YYYY-MM-DDTHH:mm:ssZ).")
    end_date_time: Optional[str] = Field(default=None, description="End date filter in ISO8601 format (YYYY-MM-DDTHH:mm:ssZ).")
    size: int = Field(default=10, description="Number of events to return per page.")
    page: int = Field(default=0, description="Page number to retrieve.")
    sort: Optional[str] = Field(default="relevance,desc", description="Sorting order of the search results.")

    @field_validator('start_date_time', 'end_date_time')
    @classmethod
    def validate_date_format(cls, v: Optional[str]) -> Optional[str]:
        """
        Validates that the provided datetime string conforms to the expected ISO8601 format.
        """
        # The validator only runs if the value is not None
        if v and "T" not in v:
            raise ValueError("Datetime must be in ISO8601 format (e.g., 'YYYY-MM-DDTHH:mm:ssZ').")
        return v

class TicketmasterAPITool:
    """
    Async Ticketmaster API tool to fetch events and event details.
    """

    BASE_URL = "https://app.ticketmaster.com/discovery/v2"

    def __init__(self):
        self.api_key = os.getenv("TICKETMASTER_API_KEY")
        if not self.api_key:
            raise ValueError("Ticketmaster API key is missing. Please set TICKETMASTER_API_KEY environment variable.")

    async def _make_request(self, endpoint: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Helper method to make async API requests to Ticketmaster.
        """
        params = params or {}
        params["apikey"] = self.api_key  # Add API key to request parameters
        url = f"{self.BASE_URL}/{endpoint}"

        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(url, params=params) as response:
                    response.raise_for_status()
                    return await response.json()
            except aiohttp.ClientResponseError as e:
                return {"error": f"HTTP Error {e.status}: {e.message}"}
            except aiohttp.ClientError as e:
                return {"error": f"Client error: {str(e)}"}
            except Exception as e:
                return {"error": f"Unexpected error: {str(e)}"}

    async def search_events(self, input: TicketmasterEventSearchInput) -> List[Dict[str, Any]]:
        """
        Asynchronously search for events using the Ticketmaster API.
        """
        params = {
            "keyword": input.keyword,
            "city": input.city,
            "countryCode": input.country_code,
            "classificationName": input.classification_name,
            "startDateTime": input.start_date_time,
            "endDateTime": input.end_date_time,
            "size": input.size,
            "page": input.page,
            "sort": input.sort,
        }

        # Remove None values from params
        params = {k: v for k, v in params.items() if v is not None}

        # Fetch results
        data = await self._make_request("events.json", params=params)

        # Extract event results
        events = data.get("_embedded", {}).get("events", [])
        results = []
        for event in events:
            results.append({
                "Event": event.get("name"),
                "Date": event.get("dates", {}).get("start", {}).get("localDate"),
                "Time": event.get("dates", {}).get("start", {}).get("localTime"),
                "Venue": event["_embedded"]["venues"][0].get("name") if "_embedded" in event and "venues" in event["_embedded"] else None,
                "City": event["_embedded"]["venues"][0]["city"].get("name") if "_embedded" in event and "venues" in event["_embedded"] else None,
                "Country": event["_embedded"]["venues"][0]["country"].get("name") if "_embedded" in event and "venues" in event["_embedded"] else None,
                "Url": event.get("url"),
            })

        return results

    async def get_event_details(self, event_id: str) -> Dict[str, Any]:
        """
        Retrieve details for a specific event by its ID.
        """
        data = await self._make_request(f"events/{event_id}.json")

        # Extract event details
        event = data
        return {
            "Event": event.get("name"),
            "Date": event.get("dates", {}).get("start", {}).get("localDate"),
            "Time": event.get("dates", {}).get("start", {}).get("localTime"),
            "Venue": event["_embedded"]["venues"][0].get("name") if "_embedded" in event and "venues" in event["_embedded"] else None,
            "City": event["_embedded"]["venues"][0]["city"].get("name") if "_embedded" in event and "venues" in event["_embedded"] else None,
            "Country": event["_embedded"]["venues"][0]["country"].get("name") if "_embedded" in event and "venues" in event["_embedded"] else None,
            "Url": event.get("url"),
        }

ticketmaster_tool = Tool(
    name="Ticketmaster Event Search",
    func=TicketmasterAPITool().search_events,  # Sync-compatible version
    coroutine=TicketmasterAPITool().search_events,  # Explicit async coroutine
    description="Searches for events using the Ticketmaster API.",
    args_schema=TicketmasterEventSearchInput,
)

#---------------------------------------------------------- AirBnB Tools ----------------------------------------------------------
class AirbnbSearchInput(BaseModel):
    location: str = Field(..., description="The destination city or area (e.g., 'Brooklyn' or 'New York City').")
    checkin_date: str = Field(..., description="The check-in date in YYYY-MM-DD format.")
    checkout_date: str = Field(..., description="The check-out date in YYYY-MM-DD format.")
    currency: str = Field(default="USD", description="The currency for the prices.")
    margin_km: float = Field(default=5.0, description="Size (in km) of bounding box margin.")


# Define Airbnb Scraper Tool
class AirbnbScraperTool:
    def __init__(self):
        """
        Initialize the Airbnb scraper.
        """
        self.geolocator = Nominatim(user_agent="airbnb_search")

    async def _get_dynamic_bbox(self, location_name: str, margin_km: float):
        """
        Asynchronously geocode to get lat/long, then build a bounding box around the center.
        """
        loop = asyncio.get_running_loop()
        geocode_result = await loop.run_in_executor(None, self.geolocator.geocode, location_name)

        if not geocode_result:
            raise ValueError(f"Could not geocode location: {location_name}")

        center_lat = geocode_result.latitude
        center_lng = geocode_result.longitude

        lat_margin_deg = margin_km / 111.0
        lng_margin_deg = margin_km / (111.0 * abs(math.cos(math.radians(center_lat))) + 1e-9)

        ne_lat = center_lat + lat_margin_deg
        ne_lng = center_lng + lng_margin_deg
        sw_lat = center_lat - lat_margin_deg
        sw_lng = center_lng - lng_margin_deg
        zoom_value = 10  # Adjust as needed

        return ne_lat, ne_lng, sw_lat, sw_lng, zoom_value

    async def search(self, input_data: AirbnbSearchInput) -> List[Dict]:
        """
        Asynchronously perform an Airbnb search by dynamically constructing a bounding box.
        """
        # Get bounding box + zoom for the location
        ne_lat, ne_lng, sw_lat, sw_lng, zoom_val = await self._get_dynamic_bbox(
            input_data.location, input_data.margin_km
        )

        # Call pyairbnb.search_all asynchronously
        loop = asyncio.get_running_loop()
        results = await loop.run_in_executor(
            None,
            pyairbnb.search_all,
            input_data.checkin_date,
            input_data.checkout_date,
            ne_lat,
            ne_lng,
            sw_lat,
            sw_lng,
            zoom_val,
            input_data.currency,
            ""
        )

        output = []
        for item in results:
            property_name = item.get("name", "N/A")

            # Extract nightly price
            price_info = item.get("price", {})
            unit_price = price_info.get("unit", {})
            currency_symbol = unit_price.get("currency_symbol", "")
            nightly_amount = unit_price.get("amount", "N/A")

            # Basic rating
            rating_info = item.get("rating", {})
            rating_value = rating_info.get("value", "N/A")

            # Construct a link from the room_id
            room_id = item.get("room_id", "")
            link = f"https://www.airbnb.com/rooms/{room_id}" if room_id else "N/A"

            output.append({
                "name": property_name,
                "price_per_night": f"{currency_symbol}{nightly_amount}",
                "rating": rating_value,
                "link": link
            })

        return output


# Define LangChain Tool
airbnb_tool = Tool(
    name="Airbnb Scraper",
    func=AirbnbScraperTool().search,
    coroutine=AirbnbScraperTool().search,  # Explicit async support
    description="Scrapes Airbnb listings based on location and check-in/check-out dates.",
    args_schema=AirbnbSearchInput
)
</file>

</files>
